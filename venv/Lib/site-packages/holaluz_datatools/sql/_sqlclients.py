import logging
import os
import re
import numpy as np
import pandas as pd
import itertools
from copy import copy
from uuid import uuid4
from abc import ABC, abstractmethod
from more_itertools import ichunked
import sqlalchemy
from sqlalchemy import create_engine
import psycopg2 # PostgreSQL
import mysql.connector as mysqlpy # MySQL
import snowflake.connector # Snowflake SQL
import snowflake.sqlalchemy 
from snowflake.connector.pandas_tools import write_pandas
from snowflake.connector.converter_null import SnowflakeNoConverterToPython
import pyodbc # SQLServer (even it may be used for them all)

from ._sqlparams import (
    FLAVOUR2CURSOR_KWARGS_MAP, FLAVOUR2LIST_TABLES_QUERY_MAP, FLAVOUR2TABLE_COLUMNS_QUERY_MAP,
    DEFAULT_TABLE_LIST_COLUMN_NAMES, 
    SNOWFLAKE_DEFAULT_SCHEMA, SNOWFLAKE_DEFAULT_STAGE, SNOWFLAKE_DEFAULT_COMPRESSION,
    SNOWFLAKE_DEFAULT_ON_ERROR, SNOWFLAKE_MAX_BATCH_SIZE, SNOWFLAKE_ACCEPTED_COPY_INTO_FTYPES,
    SNOWFLAKE_DEFAULT_COPY_INTO_FTYPE, SNOWFLAKE_DEFAULT_QUOTE_CHAR, SNOWFLAKE_DEFAULT_FAST_WRITE_KWARGS,
    SNOWFLAKE_DEFAULT_TIMEZONE
)
from ..decorators import retry, profile, compose
from ._decorators import check_connection

logger = logging.getLogger(__name__)

class InterfaceError(Exception):
    """
    Custom exception raised for errors originating from Connector/Python itself, 
    not related to the server-side
    """
    ...

class SQLClient(ABC):
    """
    Base class to interact with a SQL database from a Python environment.
    
    It must not be used directly, as its (private) method `_make_connection`
    is not defined. The user have to refer to MySQLClient, PostreSQLClient,
    SnowflakeSQLClient or SQLServerClient to stablish a connection with the 
    corresponding SQL database.
    """
    def __init__(self, host, database, username, password, port = None, account = None, role = None, warehouse = None, schema = SNOWFLAKE_DEFAULT_SCHEMA, errors = 'raise', lazy_initialization = False):
        """
        Parameters
        ----------
        host: str 
            name of the server that hosts the database
        database: str
            name of the database
        username: str
            user name used to stablish the connection
        password: str
            password for the given username
        port: int or str, optional, default: None
            port used by the database engine; only used by the MySQL and PostgreSQL clients
        account: str, optional, default: None
            connection account; only used by the Snowflake client
        role: str, optional default: None
            connection role; only used by the Snowflake client
        warehouse: str, optional, default: None
            DB warehouse; only used by the Snowflake client
        schema: str, optional, default: None
            DB schema; only used by the Snowflake client
        errors: str, optional, default: 'raise'
            how to handle errors:
            - 'raise' raises the errors
        lazy_initialization: bool, optional, default: False
            whether to connect to the DB when instantiating the class or not
            (and doing it when when making a query). it is useful when using 
            a client to only insert a table
        """
        self._host = host
        self._database = database
        self._port = port       
        self._username = username
        self._password = password

        self._account = account
        self._role = role
        self._warehouse = warehouse
        self._schema = schema
        
        self._errors = errors
        self._lazy_initialization = lazy_initialization
        
        self.client_flavour = type(self).__name__
        self.formatted_flavour = self.client_flavour.strip('Client')
        self._cursor_kwargs = FLAVOUR2CURSOR_KWARGS_MAP.get(self.client_flavour)
        self._cursors = []

        if not self._lazy_initialization:
            self.connection = self._make_connection()
            if self.connection:
                self.db_tables = self.list_db_tables()

    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_value, traceback):
        self.close_connection()

    @abstractmethod
    def _make_connection(self):
        ...

    def reconnect(self):
        """Re-open the connection with the same credentials"""
        logger.debug(f"Reconnecting")
        self.connection = self._make_connection()

    def _close_cursors(self):
        for cursor in self._cursors:
            try:
                cursor.close()
            except:
                pass

    @abstractmethod    
    def is_cursor_closed(self, cursor):
        ...

    def close_connection(self):
        """Close the DB connection"""
        # if the client has been initialized lazily, there is not need
        # to close the connection, as it has not even been opened
        if not hasattr(self, 'connection'):
            return

        self._close_cursors()

        try:
            self.connection.commit()
        except:
            logger.warning('Unable to commit DB changes before closing connection')

        self.connection.close()
        logger.debug(f"Connection to {self.formatted_flavour} DB '{self._database}' closed")

    def _handle_error(self, error, mode, show_traceback, verbose = True): 
        try:
            self.connection.rollback()
        except:
            pass
        
        if verbose:
            logger.exception(f'{type(error).__name__}: {error}', exc_info = show_traceback)

        if mode == 'raise':
            raise error

    def _fetch_records_as_dict_generator(self, cursor, chunksize):
        """fetches all the cursor data and returns it as a generator"""
        if self.is_cursor_closed(cursor):
            ie = InterfaceError('attempted to fetch from a closed cursor')
            self._handle_error(ie, mode = 'raise', show_traceback = False)

        # server-side psql cursor needs to be fetched to have a non-null
        # description
        first_row_values = cursor.fetchone()

        # check if the cursor is empty, as some queries in some flavours might 
        # yield no description, such as a DELETE in MySQL
        if first_row_values is None or cursor.description is None:
            return ()

        if hasattr(cursor, 'column_names'):
            col_names = cursor.column_names
        else:
            col_names = [desc[0] for desc in cursor.description]

        cursor_rows = itertools.chain([first_row_values], iter(cursor))
        dict_gen = (dict(zip(col_names, row)) for row in cursor_rows)

        return dict_gen if chunksize is None else ichunked(dict_gen, chunksize)

    # note that when composing decorators, order matters: here we want to check 
    # the connection before retrying to make the query, as the query can't be 
    # done if the connection is closed, and `retry` is a general-purpose decorator
    # that does not check nor open the potentially-closed connection
    @compose(retry(exceptions = psycopg2.errors.SerializationFailure, n_tries = 5), check_connection, profile)
    def _make_query(self, query, chunksize = None, verbose = True, errors = None, **cursor_kwargs):
        """
        Makes a query to the database. 
        Note this is a private method, that returns aun unformated query response,
        consider using `self.make_query` instead
        
        Parameters
        ----------
        query: str
            SQL query (use the corresponding flavour of the database)
        chunksize: int, optional, default None
            If specified, return an iterator where `chunksize` is the 
            number of rows to include in each chunk.
        verbose: bool, optional
            whether to show log messages; defaults to True
        errors: None or str, optional
            how to handle errors. if `None`, defaults to `self._errors`, 
            else 'raise' raises the errors
        **cursor_kwargs
            extra keyword arguments to pass to the connection cursor
            
        Returns
        -------
        query_data: generator[dict]
            data fetched from the DB, where each element of the generator
            is a dictionary whose keys are the column names of the queried
            table       
        """
        current_cursor_kwargs = {**self._cursor_kwargs, **cursor_kwargs}

        # psycopg2 named cursors are server-side ones, which do not load
        # the results in memory
        if 'name' in current_cursor_kwargs:
            current_cursor_kwargs['name'] = current_cursor_kwargs['name'].format(uuid4())

        try:
            current_cursor = self.connection.cursor(**current_cursor_kwargs)
            current_cursor.execute(query)
        except Exception as e:
            # we do not want `_handle_error` to log error msgs, 
            # as the `retry` decorator already does so
            self._handle_error(error = e, mode = errors, show_traceback = False, verbose = False)
            return ()
        else:
            # store the cursor so we can close it later on
            self._cursors.append(current_cursor)

            query_data = self._fetch_records_as_dict_generator(current_cursor, chunksize)

            if verbose:
                logger.debug(f"Query to {self.formatted_flavour} DB '{self._database}' successful")

            return query_data

    def _format_query_data_generator(self, query_data_gen, chunksize, return_as):
        """modifies the generator `query_data_gen` according to `chunksize` and `return_as`"""
        if return_as == 'generator':
            return query_data_gen
        elif return_as == 'list':
            if chunksize is not None:
                return (list(chunk) for chunk in query_data_gen)
            return list(query_data_gen)
        elif return_as == 'dataframe':
            if chunksize is not None:
                return (pd.DataFrame(chunk).rename(columns = lambda x: x.lower()) for chunk in query_data_gen)
            return pd.DataFrame(query_data_gen).rename(columns = lambda x: x.lower())  
        else:
            self._handle_error(
                ValueError(f"`return_as` is not one of 'dataframe', 'list' or 'generator', but '{return_as}' was given"),
                'raise',
                False
            )
         
    def make_query(self, query, return_as = 'dataframe', chunksize = None, verbose = True, errors = None, **cursor_kwargs):
        """
        Makes a query to the database and returns it formated as either 
        a `pandas.DataFrame`, a list or a generator

        Parameters
        ----------
        query: str
            SQL query (use the corresponding flavour of the database)
        return_as: str, optional
            the format to return the query data; either `'dataframe'`,
            `'list'` or `'generator'`, for a `pandas.DataFrame`, a list 
            or a generator, respectively
        chunksize: int, optional, default None
            If specified, return an iterator where `chunksize` is the 
            number of rows to include in each chunk, and each generator
            element will have `return_as` type.
        verbose: bool, optional
            whether to print feedback messages
        errors: str, optional
            how to handle errors. if `None`, defaults to `self._errors`,
            else `'raise'` raises the errors
        **cursor_kwargs
            extra keyword arguments to pass to the connection cursor
            
        Returns
        -------
        pandas.DataFrame, list or generator
            an object containg the query data, or a generator of any of them,
            depending on `return_as` and `chunksize`

        Raises
        ------
        ValueError
            if `return_as` is not one of 'dataframe', 'list' or 'generator'
        """
        errors = errors or self._errors # override self._errors, if necessary
        query_data_gen = self._make_query(query, chunksize = chunksize, verbose = verbose, errors = errors, **cursor_kwargs)
        query_data = self._format_query_data_generator(query_data_gen, chunksize, return_as)
        
        # we only expect `SELECT` queries to use chunksize, as they are the ones  
        # pulling the highest data volume, and these queries do not need to commit
        if chunksize is None:
            # TODO: make SQLServerClient cursor buffered, so we can retrieve the data
            # stored in the generators even after commiting to the connection
            if return_as != 'generator' and self.formatted_flavour != 'SQLServerClient':
                # we only need to commit when inserting, updating or deleting, but
                # not when selecting. moreover, better ask forgiveness than permission
                try:
                    self.connection.commit()
                except:
                    pass

        return query_data

    def _write_table(self, df, conn, table_name, schema, if_exists = 'fail', index = False, chunksize = None, errors = None, **kwargs):
        """
        Write the table `table_name` into the DB to which this SQL client is connected
        Parameters
        ----------
        df: pandas.DataFrame
            df that contains the table info
        url:
            SQL alchemy connection or engine
        table_name: str
            name of the table to be created
        schema: str
            schema where the table is inserted
        if_exists : str, default 'fail'
            how to behave if the table already exists, either
                - fail: raise `ValueError`
                - replace: drop the table before creating it again 
                - append: insert new values to the end of the existing table
        index: bool, optional, default: False
            whether to add an index column to the table. note that Snowflake
            does not accept indexes
        errors: str, optional
                how to handle errors:
                - 'raise' raises the errors
        kwargs
            extra keyword arguments passed to `df.to_sql`, such as `dtype`, 
            and `method`. see its docs for more info
        """
        try:
            res = df.to_sql(
                name = table_name, 
                con = conn, 
                schema = schema,
                if_exists = if_exists,
                index = index, 
                chunksize = chunksize, 
                **kwargs
            )
        except Exception as e:
            errors = errors or self._errors
            logger.error(f'Unable to create table {table_name!r} in DB {self._database!r} due to the following error')
            self._handle_error(e, mode = errors, show_traceback = False)
        else:
            success_msg = f'Table {table_name!r} created successfully in DB {self._database!r}'
            if schema:
                success_msg += f' in schema {schema!r}'
            logger.info(success_msg)
            return res

    def list_db_tables(self):
        """
        Gets the names of all tables present in the current database

        Returns
        -------
        pandas.DataFrame:
            a DataFrame containing the names of the tables and extra attributes
        """
        list_tables_query = FLAVOUR2LIST_TABLES_QUERY_MAP.get(self.client_flavour).format(self._database)
        tmp_df = self.make_query(list_tables_query, verbose = False)
        
        if tmp_df.shape[-1] == 2:
            tmp_df['table_schema'] = self._database
        
        tmp_df.columns = DEFAULT_TABLE_LIST_COLUMN_NAMES
            
        self.db_tables = tmp_df
        return tmp_df

    def _get_table_from_name(self, table_name):
        """
        Finds the table path (i.e., schema and table name)
        in the DB for the given table name.
        
        Parameters
        ----------
        table_name: str
            name of the table
            
        Returns
        -------
        table_schema_name: str
            schema and name of the table, e.g., `lead.journey`
        """
        table_name = table_name.split('.')[-1]
        tmp_df = self.db_tables[self.db_tables['table_name'] == table_name]
        schema_table_name = tmp_df['table_schema'] + '.' + tmp_df['table_name']
        
        if self.formatted_flavour in ['PostgreSQL', 'SnowflakeSQL']:
            return '"' + '"."'.join(schema_table_name.values[0].split('.')) + '"'  if not schema_table_name.empty else ''
        else:
            return schema_table_name.values[0] if not schema_table_name.empty else ''

    def table_exists(self, table_name):
        """
        Checks if a table exists in the current database
        
        Parameters
        ----------
        table_name: str
            name of the table
            
        Returns
        -------
        bool:
            whether the table exists or not
        """
        schema_table_name = self._get_table_from_name(table_name)
        if schema_table_name:
            return True
        
        logger.warning(
            f"There is no table named '{table_name}' in {self.formatted_flavour} DB '{self._database}'"
        )
        return False

        
    def get_table_col_names(self, table_name, errors = None):
        """
        Gets the columns of the table whose name is `table_name`
        
        Parameters
        ----------
        table_name: str
            name of the table
        errors: str, optional
            how to handle errors: either 'raise', 'show' or 'ignore'. it given, it overrides `self.errors`
        check_case: bool, optional
            whether to modify the case of `table_name` to automatically adjust it to the DB
            
        Returns
        -------
        col_names: pandas.Index
            the names of the table columns lowercased
        """
        if not self.table_exists(table_name):
            return pd.Index([])

        table_schema_name = self._get_table_from_name(table_name) # table_schema_name = schema.table_name
        # if there is a single schema, we don't need to prepend de database name
        full_name = f'"{self._database}".{table_schema_name}' if self.db_tables.table_schema.nunique() != 1 else table_schema_name
        query = FLAVOUR2TABLE_COLUMNS_QUERY_MAP[self.client_flavour].format(full_name)
        return self.make_query(query, errors = errors, verbose = False, return_as = 'dataframe').columns.str.lower()
    
    def get_tables_common_cols(self, table_name_list):
        """
        Returns the common columns for the given tables 
        
        Parameters
        ----------
        table_name_list: list[str]
            list of table names as `['table_name_1', 'table_name_2', ...]`
            or schema `['schema.table_name_1', 'schema.table_name_2', ...]`
            
        Returns
        -------
        col_names: pandas.Index
            names of the common columns
        """
        table_list_cols = np.asarray(
            [self.get_table_col_names(table).tolist() for table in table_name_list], 
            dtype = object
        ).sum()
        col_names, counts = np.unique(table_list_cols, return_counts = True)
        return pd.Index(col_names[counts == len(table_name_list)])
    
    def _get_table_matching_cols(self, column_name_pattern, table_name):
        """
        Searches whether there are any columns in the table `table_name` that match the regular expression pattern `column_name_pattern`
        
        Parameters
        ----------
        column_name_pattern: str or re.Pattern
            either a column name or pattern to match with a colum name
        table_name: str
            schema and name of the table
            
        Returns
        -------
        col_name_list: pandas.Index
            list of column names that match `column_name_pattern`
        """
        matching_cols = []
        for col_name in self.get_table_col_names(table_name):
            if re.search(column_name_pattern, col_name):
                matching_cols.append(col_name)

        return matching_cols

    def get_tables_with_col(self, column_name_pattern, table_name_list = None, include_col_names = False):
        """
        Returns the tables that has a column that matches `column_name_pattern`
        
        Parameters
        ----------
        column_name_pattern: str or re.Pattern
            either a column name or pattern to match with a colum name
        table_name_list: list[str], optional
            subset of tables to check. if `None`(default)
            it searches for columns matching `column_name_pattern`
            on every table in the DB
        include_col_names: bool, optional
            whether to include the names of the matching cols
            
        Returns
        -------
        table_list: pandas.Index
            list of table_path (schema.table_name)
        """
        if table_name_list is None:
            table_list = self.db_tables[['table_schema', 'table_name']].agg('.'.join, axis = 1).values
        else:
            table_list = [self._get_table_from_name(table) for table in table_name_list if self.table_exists(table)]
            
        matching_tables = []
        for table in table_list: 
            matching_cols = self._get_table_matching_cols(column_name_pattern, table)
            if matching_cols:
                matching_tables.append(table if not include_col_names else [table, matching_cols])
                
        return pd.Index(matching_tables)

class PostgreSQLClient(SQLClient): 
    """
    Python class to interact with a PostgreSQL database from a Python environment.

    It's a child class of `SQLClient`. Refer to the parent class for the implementation
    of the methods not defined here.
    """     
    def _make_connection(self):
        try:
            connection = psycopg2.connect(
                host = self._host,
                port = self._port,
                database = self._database,
                user = self._username,
                password = self._password,   
            )
        except psycopg2.OperationalError as oe:
            self._handle_error(oe, mode = self._errors, show_traceback = False)  
        except Exception as e:
            self._handle_error(e, mode = self._errors, show_traceback = True)
        else:
            logger.debug(f"Connection to {self.formatted_flavour} DB '{self._database}' successful")
            return connection

    def is_cursor_closed(self, cursor):
        """check if the given cursor is closed"""
        if cursor.closed:
            return True

        return False

    def is_connection_closed(self):
        """Check if the client connection is closed"""
        return self.connection.closed

    def get_table_col_names(self, table_name, errors = None, check_case = False):
        """
        Gets the columns of the table whose name is `table_name`
        
        Parameters
        ----------
        table_name: str
            name of the table
        errors: str, optional
            how to handle errors: either 'raise', 'show' or 'ignore'. it given, it overrides `self.errors`
        check_case: bool, optional
            whether to modify the case of `table_name` to automatically adjust it to the DB
            
        Returns
        -------
        col_names: pandas.Index
            the names of the table columns lowercased
        """
        table_name = table_name.lower() if check_case else table_name
        return super().get_table_col_names(table_name, errors)

    def write_table(self, df, table_name, schema, if_exists = 'fail', index = False, chunksize = None, errors = None, **kwargs):
        """
        Write the table `table_name` into the DB to which this SQL client is connected
        Parameters
        ----------
        df: pandas.DataFrame
            df that contains the table info
        table_name: str
            name of the table to be created
        schema: str
            schema where the table is inserted
        if_exists : str, default 'fail'
            how to behave if the table already exists, either
                - fail: raise `ValueError`
                - replace: drop the table before creating it again 
                - append: insert new values to the end of the existing table
        index: bool, optional, default: False
            whether to add an index column to the table
        errors: str, optional
                how to handle errors:
                - 'raise' raises the errors
        kwargs
            extra keyword arguments passed to `df.to_sql`, such as `dtype`, 
            and `method`. see its docs for more info
        """
        conn = create_engine(
            sqlalchemy.engine.url.URL.create(
                'postgresql+psycopg2',
                host = self._host,
                port = self._port,
                database = self._database,
                username = self._username,
                password = self._password
            )
        )
        return super()._write_table(
            df = df,
            conn = conn,
            table_name = table_name,
            schema = schema, 
            if_exists = if_exists, 
            index = index, 
            chunksize = chunksize, 
            errors = errors, 
            **kwargs
        )

class MySQLClient(SQLClient):
    """
    Python class to interact with a MySQL database from a Python environment.

    It's a child class of `SQLClient`. Refer to the parent class for the implementation
    of the methods not defined here.
    """          
    def _make_connection(self):
        try:
            connection = mysqlpy.connect(
                host = self._host,
                port = self._port,
                database = self._database,
                user = self._username,
                password = self._password
            )
        except mysqlpy.Error as e:
            self._handle_error(e, mode = self._errors, show_traceback = False)    
        except Exception as e:
            self._handle_error(e, mode = self._errors, show_traceback = True)
        else:
            logger.debug(f"Connection to {self.formatted_flavour} DB '{self._database}' successful")
            return connection

    def is_cursor_closed(self, cursor):
        """check if the given cursor is closed"""
        try:
            return not copy(cursor).close()
        except mysqlpy.InternalError:
            return False

    def is_connection_closed(self):
        """Check if the client connection is closed"""
        return not self.connection.is_connected()

    def get_table_col_names(self, table_name, errors = None, check_case = False):
        """
        Gets the columns of the table whose name is `table_name`
        
        Parameters
        ----------
        table_name: str
            name of the table
        errors: str, optional
            how to handle errors: either 'raise', 'show' or 'ignore'. it given, it overrides `self.errors`
        check_case: bool, optional
            whether to modify the case of `table_name` to automatically adjust it to the DB
            
        Returns
        -------
        col_names: pandas.Index
            the names of the table columns lowercased
        """
        table_name = table_name.upper() if check_case else table_name
        return super().get_table_col_names(table_name, errors)

    def write_table(self, df, table_name, schema, if_exists = 'fail', index = False, chunksize = None, errors = None, **kwargs):
        """
        Write the table `table_name` into the DB to which this SQL client is connected
        Parameters
        ----------
        df: pandas.DataFrame
            df that contains the table info
        table_name: str
            name of the table to be created
        schema: str
            schema where the table is inserted
        if_exists : str, default 'fail'
            how to behave if the table already exists, either
                - fail: raise `ValueError`
                - replace: drop the table before creating it again 
                - append: insert new values to the end of the existing table
        index: bool, optional, default: False
            whether to add an index column to the table
        errors: str, optional
                how to handle errors:
                - 'raise' raises the errors
        kwargs
            extra keyword arguments passed to `df.to_sql`, such as `dtype`, 
            and `method`. see its docs for more info
        """
        conn = create_engine(
            sqlalchemy.engine.url.URL.create(
                'mysql+pymysql',
                host = self._host,
                port = self._port,
                database = self._database,
                username = self._username,
                password = self._password
            )
        )
        return super()._write_table(
            df = df,
            conn = conn,
            table_name = table_name,
            schema = schema, 
            if_exists = if_exists, 
            index = index, 
            chunksize = chunksize, 
            errors = errors, 
            **kwargs
        )

class SnowflakeSQLClient(SQLClient): 
    """
    Python class to interact with a Snowflake database from a Python environment.

    It's a child class of `SQLClient`. Refer to the parent class for the implementation
    of the methods not defined here.
    """     
    def _make_connection(self):
        try:
            connection = snowflake.connector.connect(
                host = self._host,
                database = self._database,
                user = self._username,
                password = self._password, 
                account = self._account,
                role = self._role,
                warehouse = self._warehouse,
                schema = self._schema,
                converter_class = SnowflakeNoConverterToPython
            )
        except snowflake.connector.errors.OperationalError as oe:
            self._handle_error(oe, mode = self._errors, show_traceback = False)    
        except Exception as e:
            self._handle_error(e, mode = self._errors, show_traceback = True)
        else:
            logger.debug(f"Connection to {self.formatted_flavour} DB '{self._database}' successful")
            return connection
  
    def is_cursor_closed(self, cursor):
        """check if the given cursor is closed"""
        if cursor.is_closed():
            return True

        return False

    def is_connection_closed(self):
        """Check if the client connection is closed"""
        return self.connection.is_closed()

    def get_table_col_names(self, table_name, errors = None, check_case = False):
        """
        Gets the columns of the table whose name is `table_name`
        
        Parameters
        ----------
        table_name: str
            name of the table
        errors: str, optional
            how to handle errors: either 'raise', 'show' or 'ignore'. it given, it overrides `self.errors`
        check_case: bool, optional
            whether to modify the case of `table_name` to automatically adjust it to the DB
            
        Returns
        -------
        col_names: pandas.Index
            the names of the table columns lowercased
        """
        table_name = table_name.upper() if check_case else table_name
        return super().get_table_col_names(table_name, errors)

    def _write_table_fast(
        self,
        df,
        conn,
        table_name,
        schema,
        database,
        chunksize,
        errors = None,
        timezone = SNOWFLAKE_DEFAULT_TIMEZONE,
        **kwargs
    ):
        """private method that wraps `snowflake.connector.pandas_tools.write_pandas`"""
        
        # if datetime columns do not have the timezone attribute, 
        # the parquet parser processes them as UNIX timestamp  
        # objects instead of dates
        for col in df.select_dtypes(include=['datetime64[ns]']).columns:
            df[col] = df[col].dt.tz_localize(timezone)
        
        try:
            success, nchunks, nrows, _ = write_pandas(
                df = df,
                conn = conn,
                table_name = table_name,
                schema = schema,
                database = database,
                chunk_size = chunksize,
                **kwargs
            )
        except Exception as e:
            errors = errors or self._errors
            logger.error(
                f'Unable to create table {table_name!r} in DB '
                f'{self._database!r} due to the following error'
            )
            self._handle_error(e, mode = errors, show_traceback = False)
        else:
            success_msg = f'Table {table_name!r} created successfully in DB {self._database!r}'
            if schema:
                success_msg += f' in schema {schema!r}'
            logger.info(success_msg)
            return success, nchunks, nrows
        
    def write_table(
        self, 
        df, 
        table_name,
        schema,
        database = None,
        fast_write = False,
        if_exists = 'fail',
        index = False, 
        chunksize = SNOWFLAKE_MAX_BATCH_SIZE,
        errors = None, 
        **kwargs
    ):
        """
        Write the table `table_name` into the DB to which this SQL client is connected
        
        Parameters
        ----------
        df: pandas.DataFrame
            df that contains the table info
        table_name: str
            name of the table to be created
        schema: str
            schema where the table is inserted
        fast_write: bool, optional, default False
            use a writing method providing by snowflake, which is faster but
            more prone to fail
        if_exists : str, default 'fail'
            how to behave if the table already exists, either
                - fail: raise `ValueError`
                - replace: drop the table before creating it again 
                - append: insert new values to the end of the existing table
            only used if `fast_write = False`
        index: bool, optional, default: False
            whether to add an index column to the table. note that Snowflake
            does not accept indexes
        errors: str, optional
                how to handle errors:
                - 'raise' raises the errors
                - 'show' prints them. if `mode = 'show'` and `verbose = True`,
                it also prints the error traceback
        kwargs
            extra keyword arguments passed to
                - `write_pandas` if `fast = True`, such as `timezone`, used to
                  localize datetime objects; `overwrite` and `table_type`
                - `df.to_sql` otherwise, such as `dtype` and `method`
            see the corresponding docs for more info
        """
        if fast_write:
            kwargs = {**SNOWFLAKE_DEFAULT_FAST_WRITE_KWARGS, **kwargs}
            return self._write_table_fast(
                df = df,
                conn = self.connection,
                table_name = table_name,
                schema = schema, 
                database = database,
                chunksize = chunksize, 
                errors = errors,
                **kwargs
            )
        
        else:
            conn = create_engine(
                snowflake.sqlalchemy.URL(
                    account = self._account,
                    host = self._host,
                    user = self._username,
                    password = self._password,
                    warehouse = self._warehouse,
                    database = database or self._database,
                    schema = schema,
                    numpy = True
                )
            )
            
            # the snowflake Python connector is grumpy about the columns names being upper 
            # or lower cased, thus we need to upper case it, and we make a copy to not change
            # the original df
            # moreover, the connector is also grumpy about the table name being being upper 
            # or lower cased, hence, we lower case it
            return super()._write_table(
                df = df.rename(columns = lambda x: x.lower()),
                conn = conn,
                table_name = table_name.lower(),
                schema = schema, 
                if_exists = if_exists, 
                index = index, 
                chunksize = chunksize, 
                errors = errors, 
                **kwargs
            )


    def _check_s3_path(self, s3_path):
        """checks and formats the s3 path before the COPY statement"""
        if s3_path.startswith('/'):
            s3_path = s3_path[1:]
        if s3_path.endswith('/'):
            s3_path = s3_path[:-1]
        return s3_path

    def _check_filename(self, filename):
        """checks and formats the s3 filename before the COPY statement"""
        return filename.split('.', 1)[0]

    def _check_file_extension(self, file_extension):
        """checks and formats the s3 file extension before the COPY statement"""
        file_extension = file_extension or SNOWFLAKE_DEFAULT_COPY_INTO_FTYPE
        return file_extension[1:] if file_extension.startswith('.') else file_extension

    def copy_into(
        self, 
        path_s3, 
        table_name, 
        schema = SNOWFLAKE_DEFAULT_SCHEMA, 
        stage_sf = SNOWFLAKE_DEFAULT_STAGE, 
        filename = None,  
        file_extension = None,
        pattern = None, 
        compression = SNOWFLAKE_DEFAULT_COMPRESSION, 
        on_error = SNOWFLAKE_DEFAULT_ON_ERROR, 
        kms_key_id = None,
        do_truncate = True, 
        do_create_table = True, 
        df = None, 
        errors = None
    ):
        """
        Performs a (truncate and then a) copy into insertion from S3 Snowflake stage into 
        a Snowflake table `table_name` in thr schema `schema`.
        
        See the official user guide for further info and examples:
        https://docs.snowflake.com/en/user-guide/getting-started-tutorial-copy-into.html
        
        Parameters
        ----------
        path_s3: str
            path in s3 relative to the stage
        table_name: str
            snowflake table name
        schema: str, optional, default: 'PUBLIC'
            schema where the table is inserted
        stage_sf: str, optional, default: 'public.datalake'
            name of the snowflake stage integration
            note that you can also pass `schema.stage`,
            where this stage is the one where the stage
            is defined, and can be different from the 
            `schema` argument
        filename: str, optional
            s3 filename to copy into snowflake, 
            if one wants to upload only a single file
        file_extension: str, optional, default 'csv.gz'
            extension of the S3 file. must be coherent with the SF stage.
            only used when `filename` is not None
        pattern: str, optional
            s3 filename pattern to copy into snowflake, 
            if one wants to upload multiple files
        compression: str, optional, default: 'AUTO'
            compression of the S3 file, if any
            the default value `'AUTO'` detects it automatically
        on_error: str, optional, default: 'ABORT_STATEMENT'
            how Snowflake handle errors during the copy into query
        kms_key_id: str, optional
            id of the AWS KMS key used to encrypt the S3 files that are
            about the be copied
        do_truncate: bool, optional, default: True
            whether to truncate the table before the copy into statement
        do_create_table: bool, optional, default: True
            whether to create a new table before performing the copy into
            statement or not. recall that if the table does not exists, the
            copy into will fail
        df: pd.DataFrame or None, optional, default: None
            dataframe with which the table is created before the copy into
            operation. only used if `do_create_table = True`
        errors: str, optional, default: None
            how the client handles Python exceptions. is not given, defaults
            to self._errors

        NOTE: at this point this is a very opinionated function and it can
              only be used to copy ['csv', 'parquet'] files
        """
        errors = errors or self._errors

        if not bool(filename) ^ bool(pattern):
            ve = ValueError(
                'You must pass either `filename` (and optionally `file_extension`) '
                'or `pattern`, but not both nor none of them'
            )
            self._handle_error(ve, 'raise', False)
            
        if file_extension is not None and not file_extension.startswith(SNOWFLAKE_ACCEPTED_COPY_INTO_FTYPES):
            ve = ValueError(
                f'at the moment, this method can only be used to copy from '
                f'{list(SNOWFLAKE_ACCEPTED_COPY_INTO_FTYPES)} files, '
                f'but {file_extension.split(".")[0]!r} was passed'
            )
            self._handle_error(ve, 'raise', False)
            
        if pattern is not None and all(f_ext not in pattern for f_ext in SNOWFLAKE_ACCEPTED_COPY_INTO_FTYPES):
            ve = ValueError(
                f'at the moment, this method can only be used to copy from '
                f'{list(SNOWFLAKE_ACCEPTED_COPY_INTO_FTYPES)} files, '
                f'but {pattern.split("[.]", maxsplit = 1)[-1]!r} was passed'
            )
            self._handle_error(ve, 'raise', False)

        table_path = f'"{self._database}"."{schema}"."{table_name}"'.upper()
        if do_create_table and not self.table_exists(table_name):
            logger.warning(f'attempting to create table {table_path!r}')
            if df is None:
                ve = ValueError(
                    'to create a non-existing table from the `copy_into` method, '
                    'you must set the `df` parameter'
                )
                self._handle_error(ve, 'raise', False)
            self.write_table(df.iloc[:1], table_name, schema, errors = 'raise')

        if self._schema is None:
            schema_query = f'USE SCHEMA {schema}'
            self.make_query(schema_query, errors = errors)

        if do_truncate:
            truncate_query = f'TRUNCATE {table_path}'
            self.make_query(truncate_query, errors = errors)

        path_s3 = self._check_s3_path(path_s3)
        path_s3_formatted = f'{path_s3}/'

        if filename:
            filename = self._check_filename(filename)
            file_extension = self._check_file_extension(file_extension)

            full_file = f'{filename}.{file_extension}'
            copy_query_files_infix = f"""
            FROM @{stage_sf}/{path_s3}/{full_file}
            """

            copy_into_msg_files_infix = f'file {full_file!r} in the path {path_s3_formatted!r}'

            is_csv_flag = file_extension.startswith('csv')
        else: # if pattern
            copy_query_files_infix = f"""
            FROM @{stage_sf}/{path_s3}/
            PATTERN = '{pattern}'
            """

            # SF regex engine uses `[]` to scape special characters
            pattern_formated = re.sub(r'\[\.\]', r'\.', pattern) 
            copy_into_msg_files_infix = (
                f'files matching the pattern {pattern_formated!r} '
                f'in the path {path_s3_formatted!r} '
            )

            is_csv_flag = 'csv' in pattern

        if is_csv_flag:
            copy_query_file_format_infix = f"""
            FILE_FORMAT = (
                TYPE = CSV
                COMPRESSION = {compression}
                SKIP_HEADER = 1 
                FIELD_OPTIONALLY_ENCLOSED_BY = '"' 
                EMPTY_FIELD_AS_NULL = TRUE 
                ESCAPE_UNENCLOSED_FIELD = None
            )
            """   
        else: # parquet files
            copy_query_file_format_infix = f"""
            FILE_FORMAT = (
                TYPE = PARQUET
                COMPRESSION = {compression}
            )
            MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE
            """
            
        if kms_key_id is not None:
            copy_query_encryption_infix = f"""
            ENCRYPTION = (
                TYPE = 'AWS_SSE_KMS'
                KMS_KEY_ID = {kms_key_id!r}
            )
            """
            copy_into_msg_files_infix += '(using a KMS key)'
        else:
            copy_query_encryption_infix = ''

        copy_into_logger_msg = (
            f'copying data from {copy_into_msg_files_infix}, in '
            f'the Snowflake stage {stage_sf!r}, into the table '
            f'{table_path}'
        )
        logger.info(copy_into_logger_msg)

        copy_query = f"""
            COPY INTO {table_path}
            {copy_query_files_infix}
            {copy_query_file_format_infix}
            {copy_query_encryption_infix}
            ON_ERROR = '{on_error}'
        """
        return self.make_query(copy_query, errors = errors)

    def upsert_data(
        self, 
        df, 
        table_name, 
        pk_cols, 
        schema = 'PUBLIC', 
        stage = 'upsert_data', 
        insert_cols = None, 
        update_cols = None, 
        datetime_cols = None, 
        dt_format = '%Y-%m-%d', 
        adjust_datetime_cols = False
    ):
        """
        Upserts, i.e., updates the existing records on `table_name`
        and adds new rows from `df` to `table_name` using the SF
        `MERGE` statement.
        
        NOTE: this method currently does not support to delete matched
        records, even if the SF `MERGE` statement does.
        
        Parameters
        ----------
        df: pd.DataFrame
            new data to upsert into the existing table
        table_name: str
            name of the table to be upserted
        pk_cols: list[str]
            list of column names used to match the rows to be upserted
        schema: str, optional
            schema where the table `table_name` is located
        stage: str, optional
            internal SF stage where the info from `df` is temporarily placed
        insert_cols: list[str], optional
            columns whose values are inserted when `pk_cols` are not matched.
            if not given, it defaults to `df.columns`
        update_cols: list[str], optional
            columns whose values are updated when `pk_cols` are matched.
            if not given, it defaults to `df.columns`
        datetime_cols: list[str], optional
            columns whose values are initially datetime objects but we convert
            to str. if not given, it defaults to all `df` columns whose dtype is
            `'datetime'`
        dt_format: str, optional
            format of the datetime objects converted to str
        adjust_data_cols: bool, optional
            whether to cast `datetime_cols` to date if they have any other dtype
        
        Returns
        -------
        upsert_result: pd.DataFrame
            summary of the MERGE statement execution
        
        Raises
        ------
        ValueError
            if any of the column names potentially passed in
            `pk_cols`, `insert_cols`, `update_cols`, `datetime_cols`
            are not one of `df` columns
        """
        # column names check
        for name in ['pk_cols', 'insert_cols', 'update_cols', 'datetime_cols']:
            cols = eval(name)
            if cols is not None and not all(col in df.columns for col in cols):
                raise ValueError(
                    f'Invalid values for the {name!r} argument. '
                    f'The accepted ones are {df.columns.tolist()} but {cols} was given.'
                )

        if insert_cols is None:
            insert_cols = df.columns
        if update_cols is None:
            update_cols = df.columns
        
        # datetime columns check
        if datetime_cols is None:
            datetime_cols = df.select_dtypes('datetime').columns.values
        else:
            actual_datetime_cols = [
                name for name, dtype in df[datetime_cols].dtypes.to_dict().items() 
                if np.issubdtype(dtype, np.datetime64)
            ]

            if not set(datetime_cols) == set(actual_datetime_cols):
                if adjust_datetime_cols:
                    for col in datetime_cols:
                        df[col] = pd.to_datetime(df[col])
                else:
                    raise ValueError(
                        f'Invalid values for the `datetime_cols` argument. '
                        f'The actual datetime columns are {actual_datetime_cols} '
                        f'but {datetime_cols} were given. If you want to convert '
                        f'the given `datetime_cols` to `datetime` objects, set '
                        f'`adjust_datetime_cols = True`'
                    )
        
        # TODO: use _write_table_fast creating a temporary table and merge using it
        
        # convert datetime colums to str, otherwise SF messes with it when upserting
        for col in datetime_cols:
            df[col] = df[col].dt.strftime(dt_format)

        # upload the data to be upserted to a SF internal stage, 
        # while creating two temporary files (local and staged)
        filename = f'{table_name}.json'
        filepath = os.path.abspath(filename)
        df.to_json(filename, orient = 'records', lines = True, date_unit = 's', default_handler = str)
        self.make_query(f"""PUT file://{filepath} @{stage} OVERWRITE = TRUE""")

        # upsert data (using the MERGE statement)
        insert_cols_tmp_json = ', '.join([f'$1:{col} AS {col}' for col in insert_cols])
        merge_pk_columns_match = ' AND '.join([f't.{col} = {table_name}.{col}' for col in pk_cols])
        update_columns_match = ','.join([f'{col} = t.{col}' for col in update_cols])
        insert_columns_str = ','.join(insert_cols)
        insert_columns_values = ','.join([f't.{col}' for col in insert_cols])
        upsert_query = f"""
            MERGE INTO {schema}.{table_name}
            USING (
                SELECT {insert_cols_tmp_json} FROM @{stage}/{filename}
            ) AS t ON ({merge_pk_columns_match})
            WHEN MATCHED THEN UPDATE SET {update_columns_match}
            WHEN NOT MATCHED THEN INSERT ({insert_columns_str}) VALUES ({insert_columns_values})
        """

        logger.info(
            f'Upserting {df.shape[0]} rows of the table {table_name!r}, '
            f'in DB {self._database!r} and schema {schema!r}'
        )
        upsert_results = self.make_query(upsert_query)

        # remove temporary files
        self.make_query(f'remove @{stage}/{filename}')
        os.remove(filepath)

        return upsert_results

    def create_view(self, view_name, select_statement, replace = False, schema = None, database = None):
        """
        Convenience method to create the view `view_name` using the statement `select_statement`

        Parameters
        ----------
        view_name: str
            the name of the view. it can be quoted if necessary, i.e.,
            `view_name = '"quoted view name"'
        select_statement: str
            the (sub)query used to create the view
        replace: bool, optional
            whether to replace an existign view or not
        schema: str, optional
            the schema where the view is created. as `view_name`, you
            can quote it. if not given, it defaults to `self._schema`
        database: str, optional = None
            the database where the view is created. as `view_name`, you
            can quote it. if not given, it defaults to `self._database`

        Returns
        -------
        create_view_result: pd.DataFrame
            a DataFrame containing the result of the create view query
        """
        if replace:
            replace_statement = 'OR REPLACE '
            log_msg_prefix = 'replacing'
        else:
            replace_statement = ''
            log_msg_prefix = 'creating'

        # define the full table name, i.e., db.schema.table, and prettify it
        full_view_name = f'{database or self._database}.{schema or self._schema}.{view_name}'
        if not SNOWFLAKE_DEFAULT_QUOTE_CHAR in full_view_name:
            full_view_name = full_view_name.lower()

        create_view_query = f"""CREATE {replace_statement}VIEW {full_view_name} AS ({select_statement})"""

        logger.info(f'{log_msg_prefix} view {full_view_name}')
        return self.make_query(create_view_query)

    def describe_table_columns(self, table_name, descriptions_map, overwrite = False, schema = None, database = None):
        """
        Convenience function that adds descriptions (as comments) to table columns.

        Parameters
        ----------
        table_name: str
            name of the table
        descriptions_map: dict[str: str]
            python dictionary that has the column names as keys
            and the column descriptions as values       
        overwrite: bool, optional
            whether to overwrite current column descriptions
        schema: str, optional
            schema where the table is located. if not given,
            it uses `self._schema`
        database: str, optional
            database where the table is located. if not given,
            it uses `self._database`
        """
        # define the full table name, i.e., db.schema.table, and prettify it
        full_table_name = f'{database or self._database}.{schema or self._schema}.{table_name}'
        if not SNOWFLAKE_DEFAULT_QUOTE_CHAR in full_table_name:
            full_table_name = full_table_name.lower()

        if overwrite:
            logger.warning(
                f'overwritting the descriptions of all columns of table {full_table_name!r}'
            )
            actual_descriptions_map = descriptions_map
        else:
            description_df = self.make_query(f'describe table {full_table_name}')
            cols_to_describe = description_df.loc[description_df['comment'].isna(), 'name'].str.lower()
            actual_descriptions_map = {name: descriptions_map[name] for name in cols_to_describe}

        if not actual_descriptions_map:
            logger.info(f'there are no new columns to describe in the table {full_table_name!r}')
            return

        logger.info(
            f'adding the description of columns {list(actual_descriptions_map.keys())} '
            f'to table {full_table_name!r}'
        )
        for column, description in actual_descriptions_map.items():
            table_column = f'{full_table_name}.{column}'
            self.make_query(f'COMMENT ON COLUMN {table_column} IS {description!r}') 

class SQLServerClient(SQLClient): 
    """
    Python class to interact with a SQL Server database from a Python environment.
    It's a child class of `SQLClient`. Refer to the parent class for the implementation
    of the methods not defined here.
    """     
    def _make_connection(self):
        try:
            connection = pyodbc.connect("Driver={SQL Server};"
                                 "Server=" + self._host + ";"
                                 "Database=" + self._database + ";"
                                 "uid=" + self._username + ";pwd=" + self._password +"")
        except pyodbc.Error as oe:
            self._handle_error(oe, mode = self._errors, show_traceback = False)    
        except Exception as e:
            self._handle_error(e, mode = self._errors, show_traceback = True)
        else:
            logger.debug(f"Connection to SQL Server DB '{self._database}' successful")
            return connection
    
    def is_cursor_closed(self, cursor):
        """check if the given cursor is closed"""
        try:
            cursor.description
        except AttributeError:
            return True
        else:
            return False

    def is_connection_closed(self):
        """Check if the client connection is closed"""
        try:
            self.connection.cursor()
        # if the connection is None, it does not have cursor and it raises `AttributeError`,
        # and if the connection is closed, it can't use it to generate a cursor
        except (AttributeError, pyodbc.ProgrammingError):
            return True
        else:
            return False

    def get_table_col_names(self, table_name, errors = None, check_case = False):
        """
        Gets the columns of the table whose name is `table_name`
        
        Parameters
        ----------
        table_name: str
            name of the table
        errors: str, optional
            how to handle errors: either 'raise', 'show' or 'ignore'. it given, it overrides `self.errors`
        check_case: bool, optional
            whether to modify the case of `table_name` to automatically adjust it to the DB
            
        Returns
        -------
        col_names: pandas.Index
            the names of the table columns lowercased
        """
        table_name = table_name.lower() if check_case else table_name
        return super().get_table_col_names(table_name, errors)
