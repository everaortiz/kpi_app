import re
import inspect
import logging
import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)

def flatten(json_obj, parent_key = '', sep = '.'):
    """
    Turn a nested json object `json_obj` into a flattened one
    
    Parameters
    ----------
    json_obj: dict (json) 
        The json object to flatten
    parent_key: str, optional [default = '']
        The string to prepend to json_obj's keys
    sep: str, optional
        The string used to separate flattened keys
        
    Returns
    -------
    flat_json_obj: dict (json)
        A flattened json_obj
    """
    items = []
    for key, value in json_obj.items():
        new_key = str(parent_key) + sep + key if parent_key else key
        if isinstance(value, dict):
            if not value.items():
                items.append((new_key, None))
            else:
                items.extend(flatten(value, new_key, sep).items())
        elif isinstance(value, list):
            n_items = len(value)
            if n_items == 0:
                items.append((new_key, None))
            elif n_items == 1:
                items.extend(flatten(value[0], new_key, sep).items())
            else:
                for k, v in enumerate(value):
                    items.extend(flatten({str(k + 1): v}, new_key).items())
        else:
            items.append((new_key, value))
    return dict(items)

CUPS_PATTERN = re.compile(r'ES\d{16}[A-Z]{2}\d?[CFPRXYZ]?')
NUM2CUPS_CHECK_CHAR_MAP = dict(
    zip(range(23), re.sub(r'\s+', ' ', 'T R W A G M Y F P D X B N J Z S Q V H L C K E').split())
)
def is_cups_valid(cups, cups_pattern = CUPS_PATTERN, cups_check_char_map = NUM2CUPS_CHECK_CHAR_MAP):
    """checks if a cups is valid (verifying its format and control characters)"""
    cups = cups[:20] # discard last 2 (optional) characters

    if len(cups) != 20:
        return False
    
    if not (cups[0:2].isalpha() and cups[2:18].isdigit() and cups[18:20].isalpha()):
        return False

    if not cups_pattern.match(cups):
        return False
    
    n_cups = int(cups[2:18])
    r0 = n_cups%529
    c, r = divmod(r0, 23)
    return ''.join((cups_check_char_map.get(n) for n in [c, r])) == cups[18:20]

def get_kwargs_for_funtion(f, **kwargs):
    """filters the given kwargs returning only those that fit with the function `f` arguments"""
    f_args = inspect.getfullargspec(f).args
    return {key: value for key, value in kwargs.items() if key in f_args}

def reduce_mem_usage(df):
    """
    Iterate through all the columns of a dataframe and modify the data type
    to reduce memory usage with the optimum data types.

    Parameters
    ----------
    df: DataFrame (pandas)
        The DataFrame to reduce memory

    Returns
    -------
    df: DataFrame (pandas)
        DataFrame reduced
    """
    for col in df.columns:
        col_type = df[col].dtype
        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.uint).min and c_max < np.iinfo(np.uint).max:
                    df[col] = df[col].astype(np.uint)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        elif type(df[col].iloc[0]) is str:
            df[col] = df[col].str.rstrip()
        elif 'datetime' not in col_type.name:
            df[col] = df[col].astype('category')


    return df

def _upsert_dfs(df_old, df_new, pk_cols = None, keep_first = False):
    """
    helper function to upsert (concat and update) 2 dataframes `df_old` and `df_new`.
    
    for optimal results this (private) function should not be called directly but 
    invoked by `upsert_dfs`, which also has a more detailed docstring
    """
    if pk_cols is not None:
        df_old = df_old.set_index(pk_cols)
        df_new = df_new.set_index(pk_cols)
    
    if keep_first:
        df_upserted = pd.concat([df_old, df_new[~df_new.index.isin(df_old.index)]])
    else:
        df_upserted = pd.concat([df_old[~df_old.index.isin(df_new.index)], df_new])
        
    return df_upserted.reset_index(drop = pk_cols is None)

def upsert_dfs(df_old, df_new, pk_cols = None, keep_first_cols = None):
    """
    convenience function to upsert (concat and update) two dataframes
    
    Parameters
    ----------
    df_old: pandas.DataFrame
        dataframe whose values are updated    
    df_new: pandas.DataFrame
        dataframe used to update old values and add new ones 
    pk_cols: list[str], optional
        list of columns used as index during the upsert process
    keep_first_cols: list[str], optional
        list of columns whose values are not updated. for these,
        the upsert just appends new values
        
    Returns
    -------
    upserted_df: pandas.DataFrame
        the dataframe resulting of the upsert process
        
    Raises
    ------
    KeyError
        - when `df_old` and `df_new` do not have the same columns
        - when `pk_cols` is not None and any of its elements is not 
          in `df_old` cols
        - when `keep_first_cols` is not None and any of its elements
          is not in `df_old` cols
    """
    if not (df_old.columns == df_new.columns).all():
        raise KeyError('both dataframes must have the same columns')
        
    if pk_cols is not None and not all(col in df_old.columns for col in pk_cols):
        raise KeyError(
            f'invalid primary key columns passed. the accepted values are '
            f'{df_old.columns.to_list()} but {pk_cols} was passed'
        )
    
    if keep_first_cols is not None and not all(col in df_old.columns for col in keep_first_cols):
        raise KeyError(
            f'invalid keep first column list passed. the accepted values are '
            f'{df_old.columns.to_list()} but {keep_first_cols} was passed'
        )
    
    if keep_first_cols is None:
        df_upserted =  _upsert_dfs(df_old, df_new, pk_cols)

    else:
        df_first = _upsert_dfs(df_old[keep_first_cols], df_new[keep_first_cols], pk_cols, True)
        
        keep_second_cols = df_old.columns[~df_old.columns.isin(keep_first_cols)]
        df_second = _upsert_dfs(df_old[keep_second_cols], df_new[keep_second_cols], pk_cols)

        df_upserted = pd.concat([df_first, df_second], axis = 1)
    
    return df_upserted.reindex(columns = df_old.columns)
