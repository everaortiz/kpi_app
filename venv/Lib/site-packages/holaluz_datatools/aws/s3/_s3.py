import boto3
import os
import io
import re
import pandas as pd
import logging
import gzip
import warnings
import json
from abc import ABC, abstractmethod

from ...decorators import profile


SESSION_ROLE_NAME = "hlz_bi_data-tools"
SESSION_DURATION = 43200 # seconds

STRUCTURED_DATA_EXTENSIONS = ['csv', 'xls', 'xlsx', 'parquet']
SEMISTRUCTURED_DATA_EXTENSIONS = ['json']
COMPRESSED_DATA_EXTENSIONS = ['gz'] # for csv files
ACCEPTED_EXTENSIONS = STRUCTURED_DATA_EXTENSIONS \
                    + SEMISTRUCTURED_DATA_EXTENSIONS \
                    + COMPRESSED_DATA_EXTENSIONS
FTYPE2ACCEPTED_COMPRESSION_MAP = {
    'csv': ['gzip', None],
    'parquet': ['gzip', None],
    'xls': [None], 
    'xlsx': [None], 
    'json': [None]
}
ACCEPTED_ADJUST_REGEX_VALUES = ['both', 'path', 'file', False]
MAX_PAGINATION_STEP = 1000

QUOTE_CHAR = '"'
LINE_TERMINATOR = '\r\n'
CSV_DEFAULT_KWARGS = {
    'quotechar': QUOTE_CHAR, 
    'lineterminator': LINE_TERMINATOR, 
    'index': False,
}

GZIP_NEW_LINE = '\n' 


logger = logging.getLogger(__name__)

class S3Base(ABC):
    """
    Abstract class that defines the innermost methods
    of the S3Resource class
    """
    def __init__(
        self, 
        access_key, 
        secret_key, 
        assumed_role = None, 
        session_role_name = SESSION_ROLE_NAME, 
        session_duration = SESSION_DURATION
    ):
        """      
        Parameters
        ----------
        access_key: str
            access key ID 
        secret_key: str
            secret key ID
        assumed_role: str, optional
            AWS full assumed role, if any
        session_role_name: str, optional
            name of the session role (one can filter using them).
        session_duration: int, optional
            duration of the session, in seconds
        """
        # whe make accessible the high-level object-oriented interface 
        # `boto3.resource` via the `self.inner_resource` attribute and
        # the low-level object-oriented interface via `self.inner_client`
        self.session_role_name = session_role_name
        self.session_duration = session_duration
        self.max_pagination_step = MAX_PAGINATION_STEP
        
        self.inner_resource = self.login(
            access_key, 
            secret_key, 
            assumed_role, 
            self.session_role_name, 
            self.session_duration
        )
        self.inner_client =  self.inner_resource.meta.client
        
        self.update_bucket_list()
        
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_value, traceback):
        # nothing to do here, as there is no connection to close
        # because we interact with the AWS cli API
        pass

    @abstractmethod    
    def login(self, access_key, secret_key):
        ...

    @abstractmethod
    def _get_buckets_name(self):
        ...
        
    @abstractmethod
    def list_bucket_files(self):
        ...
    
    def update_bucket_list(self):
        """Updates the buckets list, and stores it in `self.buckets_name_list`"""
        self.buckets_name_list = self._get_buckets_name()
        return self.buckets_name_list
        
    def _check_bucket(self, bucket):
        """
        Checks if the input bucket name `bucket`
        is one of `self.buckets_name_list`, which is
        the result of `self._get_buckets_name`
        
        Raises `ValueError` if `bucket` is not 
        one of `self.buckets_name_list`
        """
        if bucket not in self.buckets_name_list:
            ve = ValueError((
                f"Invalid bucket name {bucket!r}. "
                f"See `self.buckets_name_list` for the list of accessible "
                f"buckets according to the session assumed role")
            )
            logger.error(f'{ve.__class__.__name__}: {ve}')
            raise ve
    
    def _check_s3_path(self, path_s3):
        """
        checks the syntax of the S3 path to make sure it does not start
        nor end with the delimiter '/'
        """
        if path_s3.startswith('/'):
            logger.debug('removing the leading character \'/\' from the S3 path')
            path_s3 = path_s3[1:]  
        if path_s3.endswith('/'):
            logger.debug('removing the trailing character \'/\' from the S3 path')
            path_s3 = path_s3[:-1]
        return path_s3
    
    def _check_ftype(self, ftype, obj = None, on_error = 'adjust'):
        """
        Checks if the file being pulled is either a `csv`, a 
        `parquet` or a `JSON` file, using the obj `obj` as a 
        helper. 
        
        Raises ValueError if the file type is not one of them.
        """
        if ftype:
            ftype = ftype.lower()

            if '.' in ftype:
                *_, real_ftype =  ftype.split('.')
            else:
                real_ftype = ftype

            if all(not ext in real_ftype for ext in ACCEPTED_EXTENSIONS):
                ve = ValueError(
                    f'Unsupported file type {real_ftype!r}. The permitted formats are {ACCEPTED_EXTENSIONS}. '
                    f'Consider using lower-level functions (via `self.inner_resource` or `self.inner_client`), '
                    f'or another file format'
                )
                logger.error(f'{ve.__class__.__name__}: {ve}')
                raise ve

            if ftype.startswith('.'):
                logger.debug('removing the leading character \'.\' from the file extension')
                ftype = ftype[1:]
            
            if isinstance(obj, dict) and ftype not in SEMISTRUCTURED_DATA_EXTENSIONS:
                warning_or_error_message = f'invalid ftype {ftype!r} for a JSON-like obj. '
                if on_error == 'raise':
                    ve = ValueError(warning_or_error_message)
                    logger.error(f'{ve.__class__.__name__}: {ve}')
                    raise ve
                else: # on_error = 'adjust'
                    warning_or_error_message += f'changing it to {SEMISTRUCTURED_DATA_EXTENSIONS[0]!r}'
                    logger.warning(warning_or_error_message)
                    return SEMISTRUCTURED_DATA_EXTENSIONS[0]
                
        return ftype
    
    def _check_compression(self, ftype, compression, on_error = 'adjust'):
        """
        checks if compression and ftype match, according to `FTYPE2ACCEPTED_COMPRESSION_MAP`
        
        """
        ftype, _ = os.path.splitext(ftype)
        if ftype in FTYPE2ACCEPTED_COMPRESSION_MAP and compression not in FTYPE2ACCEPTED_COMPRESSION_MAP[ftype]:
            warning_or_error_message = (
                f'invalid compression {compression!r} for file type {ftype!r}. '
                f'the accepted ones are {FTYPE2ACCEPTED_COMPRESSION_MAP[ftype]}. '
            )
            if on_error == 'raise':
                ve = ValueError(warning_or_error_message)
                logger.error(f'{ve.__class__.__name__}: {ve}')
                raise ve
            else: # on_error = 'adjust'
                compression = FTYPE2ACCEPTED_COMPRESSION_MAP[ftype][0] # the first is the default value
                warning_or_error_message += f'changing it to {compression!r}'
                logger.warning(warning_or_error_message)
                return compression
        return compression
                
    def _df_to_bytes_buffer(self, df, ftype, compression, **kwargs):
        """
        stores a `pandas.DataFrame` object into a `io.BytesIO` object
        to be further uploaded to a S3 bucket
        
        NOTE: pandas writer functions do not use the `compression` parameter
              when writing data to a stream, so there is no point using it 
        """
        bytes_buffer = io.BytesIO()
        if ftype.startswith('parquet'):
            parquet_kwargs = {'index': False,  **kwargs}
            df.to_parquet(bytes_buffer, **parquet_kwargs)
        elif ftype.startswith('xls'):
            xl_kwargs = {'index': False, **kwargs}
            df.to_excel(bytes_buffer, **xl_kwargs)
        else:
            csv_kwargs = {**CSV_DEFAULT_KWARGS, **kwargs}
            if not compression:
                df.to_csv(bytes_buffer, **csv_kwargs)
            else:
                with gzip.GzipFile(mode = 'w', fileobj = bytes_buffer) as gz_file:
                    df.to_csv(
                        io.TextIOWrapper(gz_file, 'utf-8', line_buffering = False, newline = GZIP_NEW_LINE), 
                        **csv_kwargs
                    )

        return bytes_buffer
    
    @abstractmethod
    def push_to_s3(self):
        ...
    
    def _file_body_to_df(self, file_obj, file_name, compression, **kwargs):
        """
        Gets a file `file` body and downloads and process it as a `pd.DataFrame` object
        
        Parameters
        ----------
        file_obj: s3.ObjectSummary
            item whose content is read and downloaded
        file_name: str
            object filename used to infer the file type
        compression: {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, 
            type of on-the-fly decompression of the given object `obj`.
            set it to `None` for no decompression
            only usd if ftype is `csv`
        kwargs:
            other keyword arguments passed to the corresponding pandas reader function 
            (`read_parquet`, `read_excel` or `read_csv`, depending on `ftype`)
           
        Return
        ------
        df: pd.DataFrame or None
            returns the object `obj` body as a dataframe, or None if the body is empty
        """
        stream = file_obj.get()['Body'].read()
        bytes_buffer = io.BytesIO(stream)
        try:
            if '.parquet' in file_name:
                return pd.read_parquet(bytes_buffer, **kwargs)
            elif '.xls' in file_name:
                return pd.read_excel(bytes_buffer, **kwargs)
            else: # assume csv
                csv_kwargs = {'compression': compression, 'quotechar': QUOTE_CHAR, **kwargs}
                return pd.read_csv(bytes_buffer, **csv_kwargs) 
        except pd.errors.EmptyDataError:
            pass
        except Exception as e:
            logger.warning(f'unable to get the file {file_name!r} due to the following error: {e.__repr__()!r}')
    
    @abstractmethod
    def pull_from_s3(self):
        ...

class S3Resource(S3Base):
    """
    Python class to interact with AWS S3 service, using
    both the main account and the BI sub-account, namely
    the BI "AdminRole" assumed role.

    It internally uses a s3 `boto3.resource` object,
    which is a high-level API. 
    """
            
    def login(self, access_key, secret_key, assumed_role = None, session_role_name = None, session_duration = None):
        """
        Logins into AWS BI account with Admin role.
        
        Parameters
        ----------
        access_key: str
            access_key 
        secret_key: str
            secret_key
        assumed_role: str, optional
            AWS full assumed role, if any
            
        Returns
        -------
        inner_resource: s3.resource
            Is used to perform operations in S3.
        """
        if assumed_role is None:
            return boto3.resource(
                's3', 
                aws_access_key_id = access_key,
                aws_secret_access_key = secret_key,
            )
            
        sts_client = boto3.client(
            'sts', 
            aws_access_key_id = access_key, 
            aws_secret_access_key = secret_key
        )
        assumed_role_object = sts_client.assume_role(
            RoleArn = assumed_role,
            RoleSessionName = session_role_name or self.session_role_name,
            DurationSeconds = session_duration or self.session_duration
        )
        
        return boto3.resource(
            's3', 
            aws_access_key_id = assumed_role_object['Credentials']['AccessKeyId'],
            aws_secret_access_key = assumed_role_object['Credentials']['SecretAccessKey'],
            aws_session_token = assumed_role_object['Credentials']['SessionToken']
        )
    
    def list_buckets(self):
        """
        Returns a `s3.bucketsCollection` with all buckets accessible
        using `self.inner_resource` with the given role
        """
        return self.inner_resource.buckets.all()

    def _get_buckets_name(self):
        """
        Returns a list containing the names of all buckets accessible
        using `self.inner_resource` with the given role
        """
        return [bucket.name for bucket in self.list_buckets()]
    
    def list_bucket_files(self, bucket, path = '', delimiter = ''):
        """
        Inventories the objects in the bucket `bucket`.
        
        NOTE: according to the following github issue,
              https://github.com/boto/boto3/issues/134#issuecomment-116766812,
              the `delimiter` argument do not map into the object 
              resource interface (the one we currently use), so we
              leave this parameter for legacy purposes but do not 
              actually use it
        Parameters
        ----------
        bucket: str
            name of the bucket whose files are listed
        path: str, optional, default: ''
            limits the response to keys that begin with the specified prefix.
            by default, gets all files in the bucket
        delimiter: str, optional, default: ''
            a character you use to group keys. we currently do not use this
            argument in the underlying code (see the note above)

        Returns
        -------
        bucket_objects: s3.Bucket.objectsCollection
            an iterable containing the (potentially filtered) items inside
            the bucket `bucket` as `s3.ObjectSummary` objects
        """
        self._check_bucket(bucket)
        if path:
            path = self._check_s3_path(path)
        
        bucket = self.inner_resource.Bucket(bucket)
        return bucket.objects.filter(
            Prefix = path, 
            # again, we do not use Delimiter ATM
            #Delimiter = delimiter
        )   
    
    def _adjust_s3_key(self, adjust_regex, path_s3 = '', s3_key_pattern = None, filename_or_pattern = None, ftype = None):
        """
        Combines and adapts the given inputs into a S3 key according to `adjust_regex`.
        
        NOTE: either `s3_key_pattern` or `filename_or_pattern` and `ftype`
        must be given as inputs
        
        Parameters
        ----------
        adjust_regex: one of {'both', 'path', 'file', False}
            how to combine the given inputs so the resulting regex
            pattern matches as it is suposed to do. there are 4
            different cases:
            - if 'both': place wildcard matches at every junction, i.e., 
                  `f'{path_s3}/.*{filename_or_pattern}.*.{ftype}'`
                  `f'{path_s3}/.*{s3_key_pattern}'`
            - if 'path': put it only between path and file part
                  `f'{path_s3}/.*{filename_or_pattern}.{ftype}'`
                  `f'{path_s3}/.*{s3_key_pattern}'`
            - if 'file': modify only the file part
                  `f'{filename_or_pattern}.*.{ftype}'`
            - if False: add no wildcard match
        path_s3: str, optional
            path inside the bucket where to perform the put
        s3_key_pattern: str or None, optional
            pattern (path/file or only file) of a S3 object key.
            if the key contains the path part (i.e., contains
            a `/`), `path_s3` is not used.
        filename_or_pattern: str or None, optional
            pattern of a S3 file object, without the file extension
            nor the path part. if passed, one must set `ftype` too
        ftype: str or None, optional
            file extension of the S3 object. if passed, one must set 
            `filename_or_pattern` too            
        adjust_regex: bool, optional
            

        Returns
        -------
        s3_key: str
        """
        assert adjust_regex in ACCEPTED_ADJUST_REGEX_VALUES, \
            f'`adjust_regex` can only be one of {ACCEPTED_ADJUST_REGEX_VALUES}'
        assert (s3_key_pattern is not None) ^ ((filename_or_pattern is not None) & (ftype is not None)), \
            'one must pass either `s3_key_pattern` or `filename_or_pattern` and `ftype`'

        if filename_or_pattern is not None:
            ftype = re.sub('\.', r'\.', ftype)
            
            if path_s3:
                if adjust_regex == 'both':
                    return f'{path_s3}/.*{filename_or_pattern}.*\.{ftype}'
                elif adjust_regex == 'path':
                    return f'{path_s3}/.*{filename_or_pattern}\.{ftype}'
                elif adjust_regex == 'file':
                    return f'{path_s3}/{filename_or_pattern}.*\.{ftype}'
                else:
                    return f'{path_s3}/{filename_or_pattern}\.{ftype}'
            else:
                if adjust_regex in ['file', 'both']:
                    return f'{filename_or_pattern}.*\.{ftype}'
                else:
                    return f'{filename_or_pattern}\.{ftype}'

        else: # s3_key_pattern is not None
            if path_s3 and '/' not in s3_key_pattern:
                if adjust_regex in ['path', 'both']:
                    return f'{path_s3}/.*{s3_key_pattern}'
                else:
                    return f'{path_s3}/{s3_key_pattern}'
            else: 
                return s3_key_pattern
        
    def filter_bucket_files(self, bucket, s3_key_pattern, path_s3 = '', adjust_regex = 'path'):
        """
        Filters the files in the `bucket` bucket according to the `s3_key_pattern` regex

        Parameters
        ----------
        bucket: str
            bucket where to perform the put
        s3_key_pattern: str, optional, default ''
            file name or pattern to match the objects key inside the bucket `bucket`.   
        path_s3: str, optional
            path inside the bucket where to perform the put
        adjust_regex: one of {'both', 'path', 'file', False}
            how to combine the given inputs so the resulting regex
            pattern matches as it is suposed to do. there are 4
            different cases:
            - if 'both' or 'path': place wildcard matches at every junction, i.e., 
                  `f'{path_s3}/.*{s3_key_pattern}'`
            - otherwise, it has not effect

            see `self._adjust_s3_key` for a more detailed explanation

        Returns
        -------
        filtered_objs: list
            whether the bucket `bucket` contains any file that matches `filename_or_pattern`
        """
        self._check_bucket(bucket)
        path_s3 = self._check_s3_path(path_s3)

        s3_key_pattern = self._adjust_s3_key(path_s3 = path_s3, s3_key_pattern = s3_key_pattern, adjust_regex = adjust_regex)
        objs = [obj for obj in self.list_bucket_files(bucket, path = path_s3) if re.search(s3_key_pattern, obj.key)]
        
        logger.info(
            f'found {len(objs)} that match the {s3_key_pattern!r} pattern '
            f' in the {bucket!r} S3 bucket'
        )
        return objs
    
    def is_file(self, bucket, filename_or_pattern, path_s3 = '', adjust_regex = 'path'):
        """
        Checks if there is any file that matches `filename_or_pattern` in the given bucket `bucket`
        (within the path `path_s3`, if given)

        Parameters
            ----------
        bucket: str
            bucket where to perform the put
        filename_or_pattern: str, optional, default ''
            file name or pattern to match the objects key inside the bucket `bucket`.   
        path_s3: str, optional
            path inside the bucket where to perform the put
        adjust_regex: bool, optional
            whether to adjust the patter `filename_or_pattern` or not, i.e., change
            `s3_path/filename_or_pattern` to `s3_path/.*filename_or_pattern` to match
            the bucket objects even if `filename_or_pattern` is not the begining of
            the objects key.
            it is only used if `path_s3` is passed

        Returns
        -------
        is_file: bool
            whether the bucket `bucket` contains any file that matches `filename_or_pattern`

        Raises
        ------
        ValueError
            if the bucket name `bucket` is not one of `self.buckets_name_list`.
        """
        return bool(self.filter_bucket_files(bucket, filename_or_pattern, path_s3, adjust_regex))

    def _pull_json_from_s3(self, files, **kwargs):
        """
        Pulls (gets) the structured files `files` from S3 and joins them
        into a single pandas DataFrame.
        Note that this is a private method; use `self.pull_from_s3` instead

        Parameters
        ----------
        files: list[tuple[s3.ObjectSummary, str]]
            a list tuples of file objs and their corresponding name
        kwargs:
            other keyword arguments passed to `json.loads`
        """
        data = []
        for file, file_name in files:

            try:
                data.append(json.loads(file.get()['Body'].read(), **kwargs))
            except Exception as e:
                logger.warning(f'unable to get the file {file_name!r} due to the following error: {e.__repr__()!r}')
                
        return data

    def _pull_dfs_from_s3(self, files, compression, concat_axis, **kwargs):
        """
        Pulls (gets) the structured files `files` from S3 and joins them
        into a single pandas DataFrame.
        Note that this is a private method; use `self.pull_from_s3` instead

        Parameters
        ----------
        files: list[tuple[s3.ObjectSummary, str]]
            a list tuples of file objs and their corresponding name
        compression: str or None
            type of on-the-fly decompression of the given object `obj`.
            set it to `None` for no decompression.
        concat_axis: int, optional, default 0
            the axis to concatenate along, when pulling more than 1 file.
            works as `axis` argument of `pd.DataFrame.concat`: 0 (or 'index') 
            for index and 1 (or 'columns') for columns
        kwargs:
            other keyword arguments passed to the corresponding pandas reader function 
            (`read_parquet`, `read_excel` or `read_csv`, depending on the file type)
        """
        if not files:
            return pd.DataFrame()
        
        files_gen = (
            self._file_body_to_df(file_obj, file_name, compression = compression, **kwargs) 
            for file_obj, file_name in files
        )
        try:
            return pd.concat(files_gen, axis = concat_axis)
        except ValueError as ve:
            if str(ve) == 'No objects to concatenate':
                logger.warning('No objects to concatenate: they all were empty')
                return pd.DataFrame()
            raise

    def pull_from_s3(
        self, 
        bucket, 
        path_s3, 
        filename_or_pattern = '', 
        ftype = 'csv.gz', 
        compression = 'gzip', 
        concat_axis = 0, 
        adjust_regex = 'path',
        **kwargs
    ):
        """
        Pulls (an undetermined number of) files containing (semi)structured data from the given
        S3 bucket `bucket` name and stores them in either a `pd.DataFrame` or a `dict` object.
        
        To download (semi)structured data or to store the downloaded data into a local file
        use `self.download_file`.
        
        Parameters
        ----------
        bucket: str
            bucket where to perform the put
        path_s3: str
            path inside the bucket where to perform the put
        filename_or_pattern: str, optional, default ''
            file name or pattern to filter the objects inside the bucket `bucket`.
            if not given, it pulls all files with extension `ftype`.    
            NOTE: it must NOT contain the file extension, which have to be set 
                  using `ftype`
        ftype: str, optional, default 'csv.gz'
            file type (extension). note that:
                - to pull multiple files with different extensions, one can set
                  `ftype = ''`
                - to pull only `'xls'` but not `'xlsx'` files, one can set
                  `ftype = 'xls$'`, where `$` is the regex ending position 
                  character
        compression: str or None, optional, default 'gzip'
            type of on-the-fly decompression of the given object `obj`.
            the accepted values depend on `ftype`:
                - if `ftype` contains `csv`: ['gzip', None]
            set it to `None` for no decompression.
        concat_axis: int, optional, default 0
            the axis to concatenate along, when pulling more than 1 file.
            works as `axis` argument of `pd.DataFrame.concat`: 0 (or 'index') 
            for index and 1 (or 'columns') for columns.
            only used when collecting structured data
        adjust_regex: one of {'both', 'path', 'file', False}
            how to combine the given inputs so the resulting regex
            pattern matches as it is suposed to do. there are 4
            different cases:
            - if 'both': place wildcard matches at every junction, i.e., 
                  `f'{path_s3}/.*{filename_or_pattern}.*.{ftype}'`
            - if 'path': put it only between path and file part
                  `f'{path_s3}/.*{filename_or_pattern}.{ftype}'`
            - if 'file': modify only the file part
                  `f'{filename_or_pattern}.*.{ftype}'`
            - if False: add no wildcard match
            
            see `self._adjust_s3_key` for a more detailed explanation
        kwargs:
            other keyword arguments passed to the corresponding pandas reader function 
            (`read_parquet`, `read_excel` or `read_csv`) or to `json.loads`, depending
            on `ftype`
            
        Returns
        -------
        objs: pd.DataFrame or list[dict]
            the object containing the pulled data; either a df, if the data is structured,
            or a list of dicts, if the data is (semi)unstructured
        
        Raises
        ------
        ValueError
            if the bucket name `bucket` is not one of `self.buckets_name_list`.
            in fact, it is raised by `self._check_bucket`
        ValueError
            if `ftype` does not correspond to either a `csv` or a `parquet` file
            (with any type of compression, among the allowed ones)   
        """
        self._check_bucket(bucket)
        path_s3 = self._check_s3_path(path_s3)
        ftype = self._check_ftype(ftype)
        compression = self._check_compression(ftype, compression, on_error = 'adjust')
        
        s3_key_pattern = self._adjust_s3_key(
            path_s3 = path_s3,
            filename_or_pattern = filename_or_pattern,
            ftype = ftype,
            adjust_regex = adjust_regex
        )
        
        files = [
            [file, file.key] for file in self.filter_bucket_files(bucket, s3_key_pattern, path_s3, adjust_regex = False)
        ]
        
        n_files = len(files)
        if n_files:
            logger.info(
                f'pulling the {n_files} (potentially empty) matched file(s) '
                f'from the {bucket!r} S3 bucket'
            )            
        else:
            logger.warning(
                f'found no files to pull that match the given pattern '
                f'in the {bucket!r} S3 bucket'
            )
        
        if ftype in SEMISTRUCTURED_DATA_EXTENSIONS:
            return self._pull_json_from_s3(files, **kwargs)
        return self._pull_dfs_from_s3(files, compression, concat_axis, **kwargs)

    def download_file(
        self, 
        bucket, 
        key_s3, 
        path_s3 = '', 
        local_path = '', 
        local_filename = '', 
        check = True, 
        verbose = True
    ):
        """
        Downloads a file from a s3 bucket.

        Parameters
        ----------
        bucket: str
            bucket where the file to download is located
        key_s3: str
            name of the file in s3. it can be either the
            full name (e.g `path/to/file/file.extension`),
            as `self.list_bucket_files` returns, or just
            the file name (e.g., `file.extension`)
        path_s3: str, optional, default = ''
            bucket path where the file is located; it is
            only used if `key_s3` does not contain the path
            part
        local_path: str, optional, default = ''
            local path where the downloaded file is placed
        local_filename: str, optional, default: ''
            name of the local file. if not given, it uses
            the file part of `key_s3`
        check: bool, optional, default = True
            whether to check if the file the user is trying
            to download is in the bucket before trying to do
            so
        verbose: bool, optional, default = True
            whether to print log messages when downloading the
            files
        """
        self._check_bucket(bucket)
        path_s3 = self._check_s3_path(path_s3)
        if path_s3 and '/' not in key_s3:
            key_s3 = f'{path_s3}/{key_s3}'

        # we check if the file is in the bucket, because the `boto3.resource` error is just
        # a HTTPError with error code 404
        if check:
            if not any(key_s3 == obj.key for obj in self.list_bucket_files(bucket, key_s3)):
                raise FileNotFoundError(f'File with key {key_s3} not found in {bucket!r} bucket')

        if local_filename:
            local_fp = os.path.join(local_path, local_filename)
        else:
            local_fp = os.path.join(local_path, key_s3.split('/')[-1])

        if verbose:
            logger.info(f'downloading file {key_s3!r} from {bucket!r} bucket into {local_fp!r}')
        return self.inner_resource.Object(bucket, key_s3).download_file(local_fp)
        
    def download_file(
        self, 
        bucket, 
        key_s3, 
        path_s3 = '', 
        local_path = '', 
        local_filename = '',
        adjust_regex = 'path',
        check = True, 
        verbose = True
    ):
        """
        Downloads a file from a s3 bucket.

        Parameters
        ----------
        bucket: str
            bucket where the file to download is located
        key_s3: str
            name of the file in s3. it can be either the
            full name (e.g `path/to/file/file.extension`),
            as `self.list_bucket_files` returns, or just
            the file name (e.g., `file.extension`)
        path_s3: str, optional, default = ''
            bucket path where the file is located; it is
            only used if `key_s3` does not contain the path
            part
        local_path: str, optional, default = ''
            local path where the downloaded file is placed
        local_filename: str, optional, default: ''
            name of the local file. if not given, it uses
            the file part of `key_s3`
        adjust_regex: one of {'both', 'path', 'file', False}
            how to combine the given inputs so the resulting regex
            pattern matches as it is suposed to do. there are 4
            different cases:
            - if 'both' or 'path': place wildcard matches at every junction, i.e., 
                  `f'{path_s3}/.*{key_s3}'`
            - otherwise, it has not effect
            
            see `self._adjust_s3_key` for a more detailed explanation
        check: bool, optional, default = True
            whether to check if the file the user is trying
            to download is in the bucket before trying to do
            so
        verbose: bool, optional, default = True
            whether to print log messages when downloading the
            files
        """
        if check:
            self._check_bucket(bucket)
            path_s3 = self._check_s3_path(path_s3)
            key_s3 = self._adjust_s3_key(
                path_s3 = path_s3,
                s3_key_pattern = key_s3,
                adjust_regex = adjust_regex
            )
            
            # we check if the file is in the bucket, because the `boto3.resource` error is just
            # a HTTPError with error code 404
            if not any(key_s3 == obj.key for obj in self.list_bucket_files(bucket, key_s3)):
                raise FileNotFoundError(f'File with key {key_s3} not found in {bucket!r} bucket')

        if local_filename:
            local_fp = os.path.join(local_path, local_filename)
        else:
            local_fp = os.path.join(local_path, key_s3.split('/')[-1])

        if verbose:
            logger.info(f'downloading file {key_s3!r} from {bucket!r} bucket into {local_fp!r}')
        return self.inner_resource.Object(bucket, key_s3).download_file(local_fp)
        
    def download_files(self, bucket, s3_key_pattern, path_s3 = '', local_path = '', verbose = False, adjust_regex = 'path'):
        """
        Downloads the files that match `key_s3_or_pattern` from the given S3 
        bucket and stores them locally.

        Parameters
        ----------
        bucket: str
            bucket where the file to download is located
        s3_key_pattern: str
            pattern to match the S3 bucket file names.
            the downloaded files will have the same names
            as their corresponding S3 key
        path_s3: str, optional, default = ''
            bucket path where the file is located; it is
            only used if `key_s3` does not contain the path
            part
        local_path: str, optional, default = ''
            local path where the downloaded file is placed
        verbose: bool, optional, default = False
            whether to print log messages when downloading the
            individual files
        adjust_regex: one of {'both', 'path', 'file', False}
            how to combine the given inputs so the resulting regex
            pattern matches as it is suposed to do. there are 4
            different cases:
            - if 'both' or 'path': place wildcard matches at every junction, i.e., 
                  `f'{path_s3}/.*{s3_key_pattern}'`
            - otherwise, it has not effect
            
            see `self._adjust_s3_key` for a more detailed explanation
        """
        # filter bucket files does the bucket and path checks, and also adjusts the s3 key
        bucket_objs = self.filter_bucket_files(bucket, s3_key_pattern, path_s3, adjust_regex = adjust_regex)
        if not bucket_objs:
            raise FileNotFoundError(f'There is no file that matches the pattern {s3_key_pattern!r} in {bucket!r} bucket')
        
        logger.info(
            f'downloading the {len(bucket_objs)} matched files from {bucket!r} bucket'
        )
        for obj in bucket_objs:
            self.download_file(bucket, key_s3 = obj.key, local_path = local_path, verbose = verbose, check = False)

    def _push_json_to_s3(self, json_data, bucket, filepath):
        """
        Uploads (puts) a Python into a S3 bucket, as a JSON file.
        Note that this is a private method, use `self.push_to_s3` instead
        
        Parameters
        ----------
        json_data: dict
            data to push 
        bucket: str
            bucket where to perform the put
        filepath: str
            path and filename to save the `df` obj 
            
        Returns
        -------
        json_response: dict
            the response of the `self.inner_resource.put_object` operation      
        """
        obj = self.inner_resource.Object(bucket, filepath)
        return obj.put(
            Body = bytes(json.dumps(json_data).encode('UTF-8')),
            ContentType='application/json' 
        )

    def _push_df_to_s3(self, df, bucket, filepath, ftype, compression, **kwargs):
        """
        Uploads (puts) a DataFrame into a S3 bucket.
        Note that this is a private method, use `self.push_to_s3` instead
        
        Parameters
        ----------
        df: pandas.DataFrame
            pandas dataframe to push 
        bucket: str
            bucket where to perform the put
        filepath: str
            path and filename to save the `df` obj 
        ftype: str
            file type (extension)
        compression: str or None
            type of on-the-fly decompression of the given object `obj`.
            set it to `None` for no compression.
        kwargs:
            other keyword arguments passed to the corresponding pandas writer function 
            (`to_parquet`, `to_excel` or `to_csv`, depending on `ftype`)
            
        Returns
        -------
        json_response: dict
            the response of the `self.inner_resource.put_object` operation      
        """
        bytes_buffer = self._df_to_bytes_buffer(df, ftype, compression, **kwargs)
        return self.inner_resource.Object(bucket, filepath).put(Body = bytes_buffer.getvalue())
    
    @profile
    def push_to_s3(self, df_or_dict, bucket, path_s3, filename, ftype = 'csv.gz', compression = 'gzip', **kwargs):
        """
        Uploads (puts) a DataFrame or a JSON into a S3 bucket.
        
        Parameters
        ----------
        df_or_dict: pandas.DataFrame or dict
            pandas dataframe to push 
        bucket: str
            bucket where to perform the put
        path_s3: str
            path inside the bucket where to perform the put
        filename: str
            filename to save the `df_or_dict` obj 
        ftype: str, optional, default: 'csv.gz'
            file type (extension)
        compression: str or None, optional, default 'gzip'
            type of on-the-fly decompression of the given object `obj`.
            the accepted values depend on `ftype`:
                - if `ftype` contains `csv`: ['gzip', None]
            set it to `None` for no compression.
        kwargs:
            other keyword arguments passed to the corresponding pandas writer function 
            (`to_parquet`, `to_excel` or `to_csv`, depending on `ftype`)
            
        Returns
        -------
        json_response: dict
            the response of the `self.inner_resource.put_object` operation
        
        Raises
        ------
        ValueError
            if the bucket name `bucket` is not one of `self.buckets_name_list`.
            in fact, it is raised by `self._check_bucket`
        ValueError
            if `ftype` does not correspond to either a `csv` or a `parquet` file
            (with any type of compression, among the allowed ones)        
        """
        self._check_bucket(bucket)
        path_s3 = self._check_s3_path(path_s3)
        ftype = self._check_ftype(ftype, df_or_dict)
        compression = self._check_compression(ftype, compression, on_error = 'adjust')
        
        filepath = f'{path_s3}/{filename}.{ftype}' if path_s3 else f'{filename}.{ftype}'
        logger.info(f'pushing file {filepath!r} to the {bucket!r} S3 bucket')

        if ftype in SEMISTRUCTURED_DATA_EXTENSIONS:
            return self._push_json_to_s3(df_or_dict, bucket, filepath)
        return self._push_df_to_s3(df_or_dict, bucket, filepath, ftype, compression, **kwargs)

    @profile
    def upload_file(self, local_filename, bucket, local_path = '', key_s3 = '', path_s3 = ''):
        """
        Uploads a local file to a S3 bucket.

        Parameters
        ----------
        local_filename: str,
            name of the local file
        bucket: str
            bucket where the file to upload is placed
        local_path: str, optional, default = ''
            local path where the downloaded file is located
        key_s3: str
            name of the file in S3. it can be either the
            full name (e.g `path/to/file/file.extension`)
            or just the file name (e.g., `file.extension`).
            if not given, it uses `local_filename`
        path_s3: str, optional, default = ''
            path where the file is placed; it is only used
            if `key_s3` does not contain the path part
        """
        self._check_bucket(bucket)
        path_s3 = self._check_s3_path(path_s3)

        local_fp = os.path.join(local_path, local_filename)

        if not key_s3:
            key_s3 = local_filename
        if path_s3 and '/' not in key_s3:
            key_s3 = f'{path_s3}/{key_s3}'

        logger.info(f'uploading file {local_fp!r} into {key_s3!r} in {bucket!r} bucket')
        return self.inner_client.upload_file(local_fp, bucket, key_s3)

    def delete_file(self, bucket, key_s3, path_s3 = '', adjust_regex = False, check = True, verbose = True):
        """
        Deletes a file from a s3 bucket.

        Parameters
        ----------
        bucket: str
            bucket where the file to download is located
        key_s3: str
            name of the file in s3. it can be either the
            full name (e.g `path/to/file/file.extension`),
            as `self.list_bucket_files` returns, or just
            the file name (e.g., `file.extension`)
        path_s3: str, optional, default = ''
            bucket path where the file is located; it is
            only used if `key_s3` does not contain the path
            part
        adjust_regex: one of {'both', 'path', 'file', False}
            how to combine the given inputs so the resulting regex
            pattern matches as it is suposed to do. there are 4
            different cases:
            - if 'both' or 'path': place wildcard matches at every junction, i.e., 
                  `f'{path_s3}/.*{key_s3}'`
            - otherwise, it has not effect
            
            see `self._adjust_s3_key` for a more detailed explanation
        check: bool, optional, default = True
            whether to check if the file wants to delete is 
            in the bucket before trying to do so
        verbose: bool, optional, default = True
            whether to print log messages when deleting the
            files
        """
        if check:
            self._check_bucket(bucket)
            path_s3 = self._check_s3_path(path_s3)
            key_s3 = self._adjust_s3_key(
                path_s3 = path_s3,
                s3_key_pattern = key_s3,
                adjust_regex = adjust_regex
            )

            # we check if the file is in the bucket, because the `boto3.resource` error is just
            # a HTTPError with error code 404
            if not any(key_s3 == obj.key for obj in self.list_bucket_files(bucket, key_s3)):
                logger.warning(f'File with key {key_s3!r} not found in {bucket!r} bucket')

        if verbose:
            logger.info(f'deleting file {key_s3!r} from {bucket!r} bucket')
        return self.inner_resource.Object(bucket, key_s3).delete()
            
    def delete_files(self, bucket, s3_key_pattern, path_s3 = '', verbose = False, adjust_regex = False):
        """
        This method enables you to delete multiple objects from a 
        bucket using a single HTTP request.
        
        Parameters
        ----------
        bucket: str
            bucket where the file to download is located
        s3_key_pattern: str
            pattern to match the S3 bucket file names.
            the downloaded files will have the same names
            as their corresponding S3 key
        path_s3: str, optional, default = ''
            bucket path where the file is located; it is
            only used if `key_s3` does not contain the path
            part
        verbose: bool, optional, default = False
            whether to print log messages when deleting each
            individual file
        adjust_regex: one of {'both', 'path', 'file', False}
            how to combine the given inputs so the resulting regex
            pattern matches as it is suposed to do. there are 4
            different cases:
            - if 'both' or 'path': place wildcard matches at every junction, i.e., 
                  `f'{path_s3}/.*{s3_key_pattern}'`
            - otherwise, it has not effect
            
            see `self._adjust_s3_key` for a more detailed explanation
        """
        bucket_objs = self.filter_bucket_files(bucket, s3_key_pattern, path_s3, adjust_regex)
        if not bucket_objs:
            logger.warning(
                f'There is no file that matches the pattern {s3_key_pattern!r} '
                f'in {bucket!r} bucket'
            )
            return

        n_files = len(bucket_objs)
        n_deleted = 0
        for idx in range(0, len(bucket_objs), self.max_pagination_step):
            delete = {
                'Objects': [{'Key': obj.key} for obj in bucket_objs[idx:idx + self.max_pagination_step]],
                'Quiet': verbose
            }
            response = self.inner_resource.Bucket(bucket).delete_objects(Delete = delete)

            n_deleted += len(response['Deleted'])
            if verbose:
                logger.info(
                    f'deleted {n_deleted} files out of the {n_files} matched '
                    f'files from {bucket!r} bucket'
                )
                
        if not verbose:
            logger.info(
                f'deleted {n_deleted} files out of the {n_files} matched '
                f'files from {bucket!r} bucket'
            )
        
    def move_file(
        self, 
        src_obj_or_key, 
        dst_bucket_or_name, 
        src_bucket_or_name = None,
        src_path = None,
        dst_path = None,
        dst_key = None,
        delete_src = True,
        check = True,
        verbose = True
    ):
        """
        Moves (cuts, or optionally copies) the file `src_obj_or_key` from `src_bucket_or_name`
        to `dst_bucket_or_name`.

        Parameters
        ----------
        src_obj_or_key: str or boto3.resources.factory.s3.Object
            the source object to move, or its name in S3
        dst_bucket_or_name: str or boto3.resources.factory.s3.Bucket
            the destination bucket, or its name in S3
        src_bucket_or_name: str or boto3.resources.factory.s3.Bucket, optional
            the source bucket, or its name in S3
        src_path: str, optional
            the path where the source object is located, inside the source bucket
        dst_path: str, optional
            the path where the destination object is located, inside the destination bucket
        dst_key: str, optional
            the name of the destination object. if not given, it uses the name of the source
            object
        delete_src: bool, optional
            whether to remove the source file (cut) or not (copy)
        check: bool, optional, default
            whether to check if the file the user wants to move is in the bucket before trying
            to do so
        verbose: bool, optional, default
            whether to print log messages when moving the file
        """
        if isinstance(dst_bucket_or_name, str):
            self._check_bucket(dst_bucket_or_name)
            dst_bucket = self.inner_resource.Bucket(dst_bucket_or_name)
            dst_bucket_name = dst_bucket_or_name
        else:
            dst_bucket_name = dst_bucket_or_name.name

        if isinstance(src_obj_or_key, str):
            assert src_bucket_or_name is not None, (
                'if `src_obj_or_key` is an object key (i.e., a string), '
                'you need to also pass the `src_bucket_or_name` argument'
            )

            if isinstance(src_bucket_or_name, str):
                self._check_bucket(src_bucket_or_name)
                src_bucket_name = src_bucket_or_name
            else:
                dst_bucket_name = src_bucket_or_name.name

            if src_path is not None:
                src_path = self._check_s3_path(src_path)
                src_key = f'{src_path}/{src_obj_or_key}'
            else:
                src_key = src_obj_or_key

            if check:
                if not any(src_key == obj.key for obj in self.list_bucket_files(src_bucket_name, src_path or '')):
                    raise FileNotFoundError(f'File with key {src_key} not found in {src_bucket_name!r} bucket')
            src_obj = self.inner_resource.Object(src_bucket_name, src_key)
        else: # assume boto3 S3 resource Object
            src_bucket_name = src_obj_or_key.bucket_name
            src_key = src_obj_or_key.key
            src_obj = src_obj_or_key
                
        copy_source = {
            'Bucket': src_bucket_name,
            'Key': src_key
        }

        src_name = src_key.rsplit('/')[-1]
        if dst_path is not None:
            dst_path = self._check_s3_path(dst_path)
            dst_key = f'{dst_path}/{dst_key or src_name}'
        else:
            dst_key = dst_key or src_name

        operation_name = 'moving' if delete_src else 'copying'
        if verbose:
            logger.info(
                    f'{operation_name} {src_key!r} from bucket {src_bucket_name!r} '
                    f'to {dst_bucket_name!r}, as {dst_key!r}'
                )
        dst_bucket.copy(copy_source, dst_key)
        
        if delete_src:
            if verbose:
                logger.info(f'deleting {src_key!r} from {src_bucket_name!r} bucket')
            return src_obj.delete()

    def move_files(self, src_bucket, src_key_pattern, dst_bucket, delete_src = True, verbose = False):
        """
        Moves (cuts, or optionally copies) the files matching `src_key_pattern` from 
        `src_bucket` to `dst_bucket`.

        If you want to move one object alone or need more versatility, check
        `self.move_file`.

        Parameters
        ----------
        src_bucket: str
            source bucket name
        src_key_pattern: str
            pattern
        dst_bucket: str
            destination bucket name
        delete_src: bool, optional
            whether to remove the source file (cut) or not (copy)
        verbose: bool, optional, default
            whether to print log messages when moving each file
        """
        self._check_bucket(src_bucket)
        self._check_bucket(dst_bucket)
        
        bucket_objs = self.filter_bucket_files(src_bucket, src_key_pattern, adjust_regex = False)
        if not bucket_objs:
            raise FileNotFoundError(f'There is no file that matches the pattern {src_key_pattern!r} in {src_bucket!r} bucket')
        
        operation_name = 'moving' if delete_src else 'copying'
        logger.info(
            f'{operation_name} {len(bucket_objs)} files from the {src_bucket!r} bucket, '
            f'that match the pattern {src_key_pattern!r}, to the {dst_bucket!r} bucket ')
        for obj in bucket_objs:
            self.move_file(obj, dst_bucket, delete_src = delete_src, verbose = verbose, check = False)
        
class S3Client(S3Resource):
    """Dummy class for backwards compatibility"""
    def __init__(self, **kwargs):
        warnings.warn(
            'The S3Client class is deprecated. Please use the S3Resource class instead', 
            DeprecationWarning,
            stacklevel = 2
        )
        super().__init__(**kwargs)

