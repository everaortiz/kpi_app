import boto3
import logging
import datetime
import time

REGION_NAME = "eu-west-1"

SESSION_ROLE_NAME = 'data-analytics-rds'
SESSION_DURATION = 43200

MAX_RESULTS_PER_PAGE = 100
SLEEP_TIME = 120 # seconds
MAX_WAIT_TIME = 10800 # seconds
MAX_TRIES = MAX_WAIT_TIME//SLEEP_TIME
EXPORT_STATUS_COMPLETE = 'COMPLETE'
DEFAULT_SNAPSHOT_TYPE = 'automated'
MAX_SNAPSHOT_DATE_DELAY_DAYS = 1

logger = logging.getLogger(__name__)

class SnapshotNotFoundError(Exception):
    """custom exception raised when a DB snapshot is not found"""
    ...
   
class ExportStatusCheckMaxRetriesError(Exception):
    """
    custom exception raised when the maximum number of checks for
    the snapshot export completion is reached
    """
    ...
    
class ExportUnsuccessfulError(Exception):
    """
    custom error raised when a snapshot export has finished but
    not all data has been exported
    """
    ...

class RDSResource:
    """
    High-level and very opinionated wrapper around the boto3
    RDS client to easily export snapshots from DBs and DB 
    clusters. More features will come soon, as needed.
    """
    def __init__(self, access_key = None, secret_key = None, assumed_role = None):
        """
        Creates the underlying boto3 secrets manager client,
        which assumes the default role `self._assumed_role`
        
        For more information about the client creation see
        `self.login`
        
        Parameters
        ----------
        access_key: str, optional
            the AWS APIs access key
        secret_key: str, optional
            the AWS APIs secret key
        """
        self._assumed_role = assumed_role
        self._region_name = REGION_NAME
        self._session_role_name = SESSION_ROLE_NAME
        self._session_duration_seconds = SESSION_DURATION
        
        self._max_results_per_page = MAX_RESULTS_PER_PAGE
        self._sleep_time = SLEEP_TIME
        self._max_tries = MAX_TRIES
        self._export_status_complete = EXPORT_STATUS_COMPLETE
        self._default_snapshot_type = DEFAULT_SNAPSHOT_TYPE
        self._max_snapshopt_date_delay_days = MAX_SNAPSHOT_DATE_DELAY_DAYS
        
        self.inner_client = self.login(access_key, secret_key, assumed_role)
        
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_value, traceback):
        pass
        
    def login(self, access_key = None, secret_key = None, assumed_role = None):
        """        
        Parameters
        ----------
        access_key: str, optional
            the AWS APIs access key
        secret_key: str, optional
            the AWS APIs secret key
        assumed_role: str, optional
            
        Returns
        -------
        secrets_manager_client: boto3.client
        """
        if assumed_role is None:
            return boto3.client(
                service_name = 'rds',
                aws_access_key_id = access_key,
                aws_secret_access_key = secret_key,
                region_name = self._region_name
            )
            
        sts_client = boto3.client(
            service_name = 'sts',
            aws_access_key_id = access_key,
            aws_secret_access_key = secret_key,
            region_name = self._region_name
        )

        assumed_role_object = sts_client.assume_role(
            RoleArn = self.assumed_role,
            RoleSessionName = self._session_role_name,
            DurationSeconds = self._session_duration_seconds
        )

        session = boto3.session.Session(
            aws_access_key_id = assumed_role_object['Credentials']['AccessKeyId'],
            aws_secret_access_key = assumed_role_object['Credentials']['SecretAccessKey'],
            aws_session_token = assumed_role_object['Credentials']['SessionToken']
        )

        return session.client(service_name = 'rds')
    
    def _paginate(self, func, payload, response_key, paginate_all):
        """
        helper method to paginate over APIs that admit it (passed 
        via de `func` argument) using the payload `payload` and 
        according to the continuation token `next_page_token`
        """
        response_data = func(**payload)
        data_list = response_data[response_key]

        while paginate_all and 'Marker' in response_data:
            payload['Marker'] = response_data['Marker']
            response_data = func(**payload)
            data_list += response_data[response_key]

        return data_list
        
    def get_db_instances(self, db_instance_name = None, filters = [], paginate_all = True):
        """
        Returns information about the provisioned RDS instances.
        
        Parameters
        ----------
        db_instance_name: str, optional
            the user-supplied instance identifier (name). if specified, 
            it returns information from only that DB instance. this 
            parameter is not case-sensitive.
        filters: list[dict], optional
            attributes to specify one or more DB instances
        paginate_all: bool, optional
            whether to paginate over all RDS instances
            
        Returns
        -------
        db_instances: list[dict]
            list of dicts containing the metadata that describe the DB instances
            
        Usage example
        -------------
        ```
            filters = [
                {'Name': 'db-instance-id', 'Values': [db_arn_1, db_arn_2]},
                {'Name': 'dbi-resource-id', 'Values': [db_resource_id]},
                {'Name': 'domain', 'Values': [db_domain]},
                {'Name': 'engine', 'Values': [db_engine]}                
            ]
            resource.get_db_instances(
                filters = filters,
                paginate_all = True
            )
        ```
        """
        payload = {
            'Filters': filters,
            'MaxRecords': self._max_results_per_page
        }
        if db_instance_name is not None:
            payload['DBInstanceIdentifier'] = db_instance_name
        
        logger.info(
            f'getting {"all " if paginate_all else ""}DB instances data '
            f'according to the given payload: {payload}'
        )
        return self._paginate(
            func = self.inner_client.describe_db_instances,
            payload = payload,
            response_key = 'DBInstances',
            paginate_all = paginate_all
        )

    def get_db_clusters(self, db_cluster_name = None, filters = [], paginate_all = True):
        """
        Returns information about the provisioned RDS instances.
        
        Parameters
        ----------
        db_cluster_name: str, optional
            the user-supplied cluster identifier (name). if specified, 
            it returns information from only that DB cluster. this 
            parameter is not case-sensitive.
        filters: list[dict], optional
            attributes to specify one or more DB clusters
        paginate_all: bool, optional
            whether to paginate over all RDS clusters
            
        Returns
        -------
        db_clusters: list[dict]
            list of dicts containing the metadata that describe the DB clusters
            
        Usage example
        -------------
        ```
            filters = [
                {'Name': 'db-cluster-id', 'Values': [db_arn_1, db_arn_2]},
                {'Name': 'dbi-resource-id', 'Values': [db_resource_id]},
                {'Name': 'domain', 'Values': [db_domain]},
                {'Name': 'engine', 'Values': [db_engine]}                
            ]
            resource.get_db_instances(
                filters = filters,
                paginate_all = True
            )
        ```
        """
        payload = {
            'Filters': filters,
            'MaxRecords': self._max_results_per_page
        }
        if db_cluster_name is not None:
            payload['DBClusterIdentifier'] = db_cluster_name
        
        logger.info(
            f'getting {"all " if paginate_all else ""}DB clusters data '
            f'according to the given payload: {payload}'
        )
        return self._paginate(
            func = self.inner_client.describe_db_clusters,
            payload = payload,
            response_key = 'DBClusters',
            paginate_all = paginate_all
        )
    
    def list_rds_resources(self, include_db_instances = True, include_db_clusters = True):
        """
        Lists all provisioned DB instances and clusters and returns 
        their corresponding metadata
        
        Parameters
        ----------
        include_db_instances: bool, optional
            whether to include the DB instances in the result
        include_db_clusters: bool, optional
            whether to include the DB clusters in the result
            
        Returns
        -------
        rds_resources: dict[list[dict]]
            a dict with keys `db_instances` and `db_clusters`, whose
            images are list of dicts containing the metadata that 
            describe the corresponding DB resources        
        """
        rds_resources = {}
        if include_db_instances:
            rds_resources['db_instances'] = self.get_db_instances()
        if include_db_clusters:
            rds_resources['db_clusters'] = self.get_db_clusters()
        
        return rds_resources
    
    @property
    def rds_resources(self):
        return self.list_rds_resources()
    
    @property
    def rds_resource_names(self):
        rds_resource_names = {}
        for key, values_list in self.rds_resources.items():
            inner_key = 'DBClusterIdentifier' if key == 'db_clusters' else 'DBInstanceIdentifier'
            rds_resource_names[key] = [v[inner_key] for v in values_list]
        
        return rds_resource_names
    
    def get_db_snapshots(
        self, 
        db_instance_name = None, 
        db_snapshot_name = None,
        snapshot_type = DEFAULT_SNAPSHOT_TYPE,
        include_public = False,
        include_shared = False,
        filters = [],
        paginate_all = True
    ):
        """
        Returns information about the DB snapshots.
        
        Parameters
        ----------
        db_instance_name: str, optional
            the user-supplied instance identifier (name). this 
            parameter is not case-sensitive, and can not be used
            in conjunction with `db_snapshot_name`
        db_snapshot_name: str, optional
            a specific DB snapshot identifier to describe. this
            parameter can't be used in conjunction with
            `db_instance_name`
        snapshot_type: str, optional
            the type of snapshots to be returned. must be one of
            {'automated', 'manual', 'shared', 'public', ' awsbackup'}.
            if not provided, only automated and manual snapshots are
            returned
        include_public = False
            whether to include manual DB snapshots that are public and
            can be copied or restored by any AWS account
        include_shared = False
            whether to include shared manual DB snapshots from other
            AWS accounts that this AWS account has been given 
            permission to copy or restore
        filters: list[dict], optional
            attributes to specify one or more DB clusters
        paginate_all: bool, optional
            whether to paginate over all RDS clusters
            
        Returns
        -------
        db_snapshots: list[dict]
            list of dicts containing the metadata that describe the DB snapshots
            
        Usage example
        -------------
        ```
            filters = [
                {'Name': 'db-instance-id', 'Values': [db_arn_1, db_arn_2]},
                {'Name': 'db-snapshot-id', 'Values': [db_snpsht_arn_1, db_snpsht_arn_2]},
                {'Name': 'dbi-resource-id', 'Values': [db_resource_id]},
                {'Name': 'snapshot-type', 'Values': [type_1, type_2]},
                {'Name': 'engine', 'Values': [db_engine]}                
            ]
            resource.get_db_instances(
                filters = filters,
                paginate_all = True
            )
        ```
        """
        if not bool(db_instance_name) ^ bool(db_snapshot_name):
            ve = ValueError(
                'you must pass either `db_instance_name` or `db_snapshot_name`, '
                'but not none nor both or them'
            )
            logger.error(f'{ve.__class__.__name__}: {ve}')
            raise ve
            
        payload = {
            'SnapshotType': snapshot_type,
            'IncludeShared': include_shared,
            'IncludePublic': include_public,
            'Filters': filters,
            'MaxRecords': self._max_results_per_page
        }
        if db_instance_name is not None:
            payload['DBInstanceIdentifier'] = db_instance_name
        if db_snapshot_name is not None:
            payload['DBSnapshotIdentifier'] = db_snapshot_name
        
        logger.info(
            f'getting {"all " if paginate_all else ""}DB snapshots data '
            f'according to the given payload: {payload}'
        )
        return self._paginate(
            func = self.inner_client.describe_db_snapshots,
            payload = payload,
            response_key = 'DBSnapshots',
            paginate_all = paginate_all
        )

    def get_db_cluster_snapshots(
        self, 
        db_cluster_name = None, 
        db_cluster_snapshot_name = None,
        snapshot_type = DEFAULT_SNAPSHOT_TYPE,
        include_public = False,
        include_shared = False,
        filters = [],
        paginate_all = True
    ):
        """
        Returns information about the DB cluster snapshots.
        
        Parameters
        ----------
        db_cluster_name: str, optional
            the user-supplied cluster identifier (name). this 
            parameter is not case-sensitive, and can not be used
            in conjunction with `db_cluster_snapshot_name`
        db_cluster_snapshot_name: str, optional
            a specific DB cluster snapshot identifier to describe. 
            this parameter can't be used in conjunction with
            `db_cluster_name`
        snapshot_type: str, optional
            the type of snapshots to be returned. must be one of
            {'automated', 'manual', 'shared', 'public', ' awsbackup'}.
            if not provided, only automated and manual snapshots are
            returned
        include_public = False
            whether to include manual DB cluster snapshots that are 
            public and can be copied or restored by any AWS account
        include_shared = False
            whether to include shared manual DB cluster snapshots from 
            other AWS accounts that this AWS account has been given 
            permission to copy or restore
        filters: list[dict], optional
            attributes to specify one or more DB clusters
        paginate_all: bool, optional
            whether to paginate over all RDS clusters
            
        Returns
        -------
        db_cluster_snapshots: list[dict]
            list of dicts containing the metadata that describe the
            DB cluster snapshots
            
        Usage example
        -------------
        ```
            filters = [
                {'Name': 'db-cluster-id', 'Values': [db_arn_1, db_arn_2]},
                {'Name': 'db-cluster-snapshot-id', 'Values': [db_snpsht_arn_1, db_snpsht_arn_2]},
                {'Name': 'snapshot-type', 'Values': [type_1, type_2]},
                {'Name': 'engine', 'Values': [db_engine]}                
            ]
            resource.get_db_instances(
                filters = filters,
                paginate_all = True
            )
        ```
        """
        if not bool(db_cluster_name) ^ bool(db_cluster_snapshot_name):
            ve = ValueError(
                'you must pass either `db_cluster_name` or `db_cluster_snapshot_name`, '
                'but not none nor both or them'
            )
            logger.error(f'{ve.__class__.__name__}: {ve}')
            raise ve
            
        payload = {
            'SnapshotType': snapshot_type,
            'IncludeShared': include_shared,
            'IncludePublic': include_public,
            'Filters': filters,
            'MaxRecords': self._max_results_per_page
        }
        if db_cluster_name is not None:
            payload['DBClusterIdentifier'] = db_cluster_name
        if db_cluster_snapshot_name is not None:
            payload['DBClusterSnapshotIdentifier'] = db_cluster_snapshot_name
        
        logger.info(
            f'getting {"all " if paginate_all else ""}DB clusters snapshots data '
            f'according to the given payload: {payload}'
        )
        return self._paginate(
            func = self.inner_client.describe_db_cluster_snapshots,
            payload = payload,
            response_key = 'DBClusterSnapshots',
            paginate_all = paginate_all
        )
        
    def get_snapshot_metadata(
        self, 
        db_name = None, 
        db_cluster_name = None, 
        snapshot_type = DEFAULT_SNAPSHOT_TYPE,
        snapshot_idx = -1,
        check_snapshot_date = True
    ):
        """
        Returns information about a specific DB instance or 
        DB cluster snapshot, according to `snapshot_idx`
        
        Parameters
        ----------
        db_name: str, optional
            the user-supplied DB identifier (name). this 
            parameter is not case-sensitive, and can not be used
            in conjunction with `db_cluster_name`
        db_cluster_name: str, optional
            the user-supplied DB cluster identifier (name). this 
            parameter is not case-sensitive, and can not be used
            in conjunction with `db_name`
        snapshot_type: str, optional
            the type of snapshots to be returned. must be one of
            {'automated', 'manual', 'shared', 'public', ' awsbackup'}.
            if not provided, only automated and manual snapshots are
            returned
        snapshot_idx: int, optional
            position of the target snapshot in the list of all 
            available snapshots fetched. by default, the last
        check_snapshot_date: bool, optional
            whether to check the snapshot creation date to assert we
            do not get old snapshots data
            
        Returns
        -------
        target_snapshot_metadata: dict
            dict containing the metadata of the target snapshopt
        """
        if not bool(db_name) ^ bool(db_cluster_name):
            ve = ValueError(
                f'you must set either `db_name` or `db_cluster_name` '
                f'but not both not none of them.'
            )
            logger.error(f'{ve.__class__.__name__}: {ve}')
            raise ve

        if db_name is not None:
            snapshots_metadata = self.get_db_snapshots(
                db_name = db_name,
                snapshot_type = snapshot_type
            )
        else: # db cluster
            snapshots_metadata = self.get_db_cluster_snapshots(
                db_cluster_name = db_cluster_name,
                snapshot_type = snapshot_type
            )
        
        if not snapshots_metadata:
            snfe = SnapshotNotFoundError(
                f'unable to find a snapshot for the given resource '
                f'{(db_name or db_cluster_name)!r}' 
            )
            logger.error(f'{snfe.__class__.__name__}: {snfe}')
            raise snfe
              
        target_snapshot_metadata = snapshots_metadata[snapshot_idx]
        
        if check_snapshot_date:
            # check snapshot date; we do not want to update the table with "old" data
            today_date = datetime.date.today()
            snapshot_date = target_snapshot_metadata['SnapshotCreateTime'].date()
            if (today_date - snapshot_date).days > self._max_snapshopt_date_delay_days:
                last_accepted_date = today_date - datetime.timedelta(days = self._max_snapshopt_date_delay_days)
                snfe = SnapshotNotFoundError(
                    f'unable to find a snapshot created within the accepted date range. '
                    f'the most recent accepted date is {last_accepted_date}, but the '
                    f'target snapshot was created on {snapshot_date}.'
                )
                logger.error(f'{snfe.__class__.__name__}: {snfe}')
                raise snfe
        
        return target_snapshot_metadata
    
    def export_snapshopt(
        self, 
        export_id, 
        iam_role_arn, 
        snapshot_arn,  
        kms_key_id,
        s3_bucket_name,
        export_only = None,
        s3_prefix = None,
        wait_till_complete = True
    ):
        """
        Starts an export of a snapshot to Amazon S3.
        
        Parameters
        ----------
        export_id: str
            a unique identifier for the snapshot export task
        iam_role_arn: str
            the name of the IAM role to use for writing to the
            S3 bucket when exporting a snapshot
        snapshot_arn: str
            the arn of the snapshot to export to S3
        kms_key_id: str
            the ID of the AWS KMS customer master key (CMK) to
            use to encrypt the snapshot exported to S3
        s3_bucket_name: str
            the name of the S3 bucket to export the snapshot to
        export_only: str or list[str], optional
            dhe data to be exported from the snapshot. if not 
            provided, all the snapshot data is exported. valid
            values syntax is: `database[.schema][.table]`
        s3_prefix: str, optional
            the S3 bucket prefix to use as the file name and path
            of the exported snapshot
        wait_till_complete: bool, optional
            whether to wait for the snapshot to finish, and log 
            update messages
            
        Returns
        -------
        current_export: dict
            dict containing the metadata of the current export
            at the moment is it completed, or at the moment the
            start task starts if `wait_till_complete = False`
        """
        payload = {
            'ExportTaskIdentifier': export_id,
            'IamRoleArn': iam_role_arn,
            'SourceArn': snapshot_arn,
            'KmsKeyId': kms_key_id,
            'S3BucketName': s3_bucket_name,
        }
        
        if export_only is not None:
            if isinstance(export_only, str):
                export_only = [export_only]
            
            payload['ExportOnly'] = export_only
        
        if s3_prefix is not None:
            payload['S3Prefix'] = s3_prefix

        payload_formatted = payload.copy()
        payload_formatted.update({'IamRoleArn': '****', 'KmsKeyId': '****',})
        logger.info(
            f'starting snapshot export process according to '
            f'the given payload: {payload}'
        )
        export_response = self.inner_client.start_export_task(**payload)

        if not wait_till_complete:
            return export_response
        
        # wait for the export to finish
        for i in range(1, self._max_tries + 1):
            exports_status = self.inner_client.describe_export_tasks(
                ExportTaskIdentifier = export_id,
                SourceArn = snapshot_arn
            )
            current_export = exports_status['ExportTasks'][-1]
            current_status = current_export['Status']
            extracted_data = current_export['TotalExtractedDataInGB']
            pct_done = current_export['PercentProgress']
            if current_status == self._export_status_complete:
                logger.info(f'export task complete: {extracted_data} GB extracted')
                return current_export

            logger.info(
                f'awaiting for the export task to finish (check {i} of {self._max_tries}). '
                f'current status: {current_status!r}, {extracted_data} GB extracted '
                f'({pct_done}% done)'
            )
            time.sleep(self._sleep_time)
        else:
            esmre = ExportStatusCheckMaxRetriesError(
                f'max retries reached while awaiting for the snapshot to complete'
            )
            logger.error(f'{esmre.__class__.__name__}: {esmre}')
            raise esmre    