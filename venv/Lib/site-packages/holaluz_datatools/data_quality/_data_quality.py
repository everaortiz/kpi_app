import os
import datetime
import pytz
import logging
import functools
import inspect

from ruamel import yaml
import great_expectations as ge
from great_expectations.expectations.expectation import ExpectationConfiguration

from ._helpers import (
    find_context_yml_file, 
    load_expectations_file, 
    load_custom_expectations,
    SEARCH_START_DIRS,
    DATA_QUALITY_DIR,
    DATA_QUALITY_DIR_NAME
)
from ..utils import search_fp_obj
from ..credentials import load_credentials
from ..slack import send_message_by_slack

FORMAT_NAME_JOIN_CHARACTER = '_'

GE_DEFAULT_CONTEXT_ATTR = 'GE_YML'

# we might not need to create new ones each time if we do not use
# other types of data, but we still need to check this
DATASOURCE_NAME = 'datasource_pandas_runtime'
DATA_CONNECTOR_NAME = 'data_connector_pandas_runtime'
BATCH_IDENTIFIER_PREFIX = 'batch_identifier_pandas_runtime'

DATA_ASSET_NAME_SUFFIX = 'data'
EXPECTATION_SUITE_NAME_SUFFIX = '{}_expectation_suite' # expectation level formattable
CHECKPOINT_NAME_SUFFIX = '{}_checkpoint' # expectation level formattable
CHECKPOINT_RUN_NAME_SUFFIX = '{}_run' # environment and expectation level formattable

CRITICAL_RESOURCE_INFIX = 'critical'
WARNING_RESOURCE_INFIX = 'warning'

TIMEZONE = 'Europe/Madrid'

PROD_ENVIRONMENT_NAME = 'prod'
TEST_ENVIRONMENT_NAME = 'test'
ACCEPTED_ENVIRONMENT_VALUES = [PROD_ENVIRONMENT_NAME, TEST_ENVIRONMENT_NAME]
ENVIRONMENT2DATA_DOCS_SITE_MAP = {
    TEST_ENVIRONMENT_NAME: 'local_site', 
    PROD_ENVIRONMENT_NAME: 's3_site'
}
ENVIRONMENT2DATA_DOCS_LOCATION_NAME = {
    TEST_ENVIRONMENT_NAME: 'file', 
    PROD_ENVIRONMENT_NAME: 'URL'
}

# default datasource config to interact with an in-memory pandas dataframe
DATASOURCE_YAML = r"""
name: {datasource_name}
class_name: Datasource
module_name: great_expectations.datasource
execution_engine:
  module_name: great_expectations.execution_engine
  class_name: PandasExecutionEngine
data_connectors:
  {data_connector_name}:
    class_name: RuntimeDataConnector
    batch_identifiers:
      - {batch_identifier}
"""

# default checkpoint config 
# it defines the action list of the potencial validation of two suites
# (critical and warning), but the validation itself is characterized
# at runtime, to allow the user to include only the suites that she
# considers necessary

# TOCONSIDER: create another checkpoint that also includes statistical drift
CHECKPOINT_YAML = """
name: {checkpoint_name}
config_version: 1
class_name: Checkpoint
action_list:
  - name: store_validation_result
    action:
      class_name: StoreValidationResultAction
  - name: store_evaluation_params
    action:
      class_name: StoreEvaluationParametersAction
  - name: update_data_docs
    action:
      class_name: UpdateDataDocsAction
      site_names: []
"""

ACCEPTED_NOTIFY_ON_KEYS = ['critical', 'warning']
ACCEPTED_NOTIFY_ON_VALUES = ['success', 'failure', 'all', 'none']

SLACK_NOTIFICATION_TEMPLATE = """
*Batch Validation Status*: {validation_status} {validation_status_emoji}
*Data asset name*: `{data_asset_name}`
*Expectation suite name*: `{expectation_suite_name}`
*Checkpoint name*: `{checkpoint_name}`
*Run ID*: `{run_id}`
*Batch ID*: `{batch_id}`

*Summary*: {n_successful_expectations} of {n_evaluated_expectations} expectations were met ({pct_successful_expectations}%)
*Data Docs* can be found in the following <{data_docs_url}|{location}>
"""

SUCCESSFUL_VALIDATION_STR = 'success'
FAILED_VALIDATION_STR = 'failure'

VALIDATION_STATUS2_EMOJI_MAP = {
    SUCCESSFUL_VALIDATION_STR: ':tada:',
    FAILED_VALIDATION_STR: ':x:'
}
ENVIRONMENTS_WITH_SLACK_NOTIFICATIONS = [PROD_ENVIRONMENT_NAME]

SLACK_CREDENTIALS_KEY = 'slack_bi_integrator'
SLACK_CHANNEL = 'data-quality-alerts'
CRITICAL_VALIDATION_NOTIFY_VALUE = 'failure'
WARNING_VALIDATION_NOTIFY_VALUE = 'failure'

logger = logging.getLogger(__name__)

# utils
def _format_name(*parts: str, join_character: str = FORMAT_NAME_JOIN_CHARACTER):
    """Helper function to create the names of the great expectations objects"""
    return join_character.join((part.lower() for part in parts))

# custom exception
class CriticalExpectationsFailedError(Exception):
    """Custom exception raised when the critical expectations are not met"""
    ...

# helper decorators
def overwrite_defaults(func):
    """
    Decorator that overwrites the attributes of the object `cls`
    according to the `kwargs` passed to the method `func`, if
    the given ones are not None
    """
    @functools.wraps(func)
    def wrapper(cls, *args, **kwargs):
        if args:
            raise SyntaxError(
                f'Use only keyword arguments when calling {func.__name__!r} method'
            )
            
        for attr_name, attr_value in kwargs.items():
            if hasattr(cls, attr_name) and attr_value is not None:
                logger.debug(
                    f'overwritting class attr: self.{attr_name} = '
                    f'{getattr(cls, attr_name)} -> self.{attr_name} '
                    f'= {attr_value}')
                setattr(cls, attr_name, attr_value)
                
        return func(cls, **kwargs)
    
    return wrapper

def use_defaults(func):
    """
    Decorator that retrieves the attributes of the object `cls`
    when the `kwargs` signature of method `func` are None
    """
    func_sig = inspect.signature(func)
    
    @functools.wraps(func)
    def wrapper(cls, *args, **kwargs):
        if args:
            raise SyntaxError(
                f'Use only keyword arguments when calling {func.__name__!r} method'
            )
        
        for arg_name, arg_value in func_sig.parameters.items():
            if arg_name not in kwargs and arg_name != 'self': 
                kwargs[arg_name] = arg_value.default
        
        for attr_name, attr_value in kwargs.items():
            if hasattr(cls, attr_name) and attr_value is None:
                logger.debug(f'using default value for self.{attr_name}: {getattr(cls, attr_name)}')
                kwargs[attr_name] = getattr(cls, attr_name)
        
        return func(cls, **kwargs)
    
    return wrapper

# main class
class DataQualityChecker:
    """
    Wrapper class that uses the `great_expectations` Python module
    to perform an opinionated data quality check. More precisely, 
    we have decided to
    - use an in-memory datasource; 
    - create the expectations suites (critical and warning) manually, 
      by passing a expectations list;
    - create the corresponding checkpoints (critical and warnings) to 
      validate the data from the ET process against the corresponding
      expectation suites;
    - store both the validation results and the data docs either in S3
      (in the `data-quality-expectations` bucket) or locally, depending
      on the `environment` variable.

    Note that we have both warnings (non-crucial) and critical (minimum 
    indispensable) expectations. While the former just sends a slack alert
    if the corresponding checkpoint is not passed, the later would also
    stop the process before the L part.
    """
    def __init__(
        self,
        base_name,
        environment,
        datasource_name = None, 
        data_connector_name = None,
        batch_identifier = None,
        expectation_suite_name = None,
        checkpoint_name = None,
        data_asset_name = None,
        run_name = None,
        notify_on = None,
        search_start_dir = None, 
        n_layers = 5,
        do_load_custom_expectations = True,
    ):
        """
        Parameters
        ----------
        base_name: str
            base name of the underlying `great_expectations` objects
        environment: str, {'prod', `test'}
            the execution environment. if
            - test: development execution mode that does not upload the
                    data docs to the S3 bucket nor send slack alerts
            - prod: production execution mode that uploads the data docs
                    to the S3 bucket, sends slack alerts and stops the 
                    ETL process if the critical expectations are not met
        datasource_name: str, optional
            `great_expectations` datasource name. if not given, it defaults
            to `DATASOURCE_NAME`
        data_connector_name: str, optional
            `great_expectations` data connector name. if not given, it defaults
            to `DATA_CONNECTOR_NAME`
        batch_identifier: str, optional
            `great_expectations` batch identifier name. if not given, it defaults
            to a combination of `BATCH_IDENTIFIER_PREFIX` and `base_name`
        expectation_suite_name: str, optional
            `great_expectations` expectation suite name. if not given, it defaults
            to a combination of `base_name` and `EXPECTATION_SUITE_NAME_SUFFIX`
        checkpoint_name: str, optional
            `great_expectations` checkpoint name. if not given, it defaults
            to a combination of `base_name` and `CHECKPOINT_NAME_SUFFIX`
        data_asset_name: str, optional
            `great_expectations` data asset name. if not given, it defaults
            to a combination of `base_name` and `DATA_ASSET_NAME_SUFFIX`
        run_name: str, optional 
            `great_expectations` checkpoint run name. if not given, it defaults
            to a combination of `base_name` and `CHECKPOINT_RUN_NAME_SUFFIX`
        notify_on: str or dict, optional
            notification mode. if
            - `str`: it must be one of `ACCEPTED_NOTIFY_ON_VALUES`
            - `dict`: its keys must belong to `ACCEPTED_NOTIFY_ON_KEYS`
                      and its values be one of `ACCEPTED_NOTIFY_ON_VALUES`
            it defaults to `CRITICAL_VALIDATION_NOTIFY_VALUE`
        search_start_dir: str, optional
            folder to start searching the YAML context files. if not given,
            it defaults to the current working directory
        n_layers: int, optional
            number of directory layers to search upwards
        do_load_custom_expectations: bool, optional
            whether to import custom expectations from the 
            `great_expectations/plugins/custom_expectations` folder or not
        """
        self._check_environment(environment)
        self.context = self.load_context(
            environment, 
            do_load_custom_expectations,
            search_start_dir, 
            n_layers
        )
                
        self.environment = environment
        self.notify_on = notify_on or CRITICAL_VALIDATION_NOTIFY_VALUE
        self.base_name = base_name.lower()
        self.datasource_name = datasource_name or DATASOURCE_NAME
        self.data_connector_name = data_connector_name or DATA_CONNECTOR_NAME
        self.batch_identifier = batch_identifier or \
            _format_name(BATCH_IDENTIFIER_PREFIX, self.base_name)
        self.expectation_suite_name = expectation_suite_name or \
            _format_name(self.base_name, EXPECTATION_SUITE_NAME_SUFFIX)
        self.checkpoint_name = checkpoint_name or \
            _format_name(self.base_name, CHECKPOINT_NAME_SUFFIX)
        self.data_asset_name = data_asset_name or \
            _format_name(self.base_name, DATA_ASSET_NAME_SUFFIX)
        self.run_name = run_name or \
            _format_name(self.base_name, CHECKPOINT_RUN_NAME_SUFFIX)
        
        self.checkpoints_to_run = None
        
    def _check_environment(self, environment):
        """Checks if `environment` is one of `ACCEPTED_ENVIRONMENT_VALUES`"""
        if environment not in ACCEPTED_ENVIRONMENT_VALUES:
            raise ValueError(
                f'the accepted values of `environment` are '
                f'{ACCEPTED_ENVIRONMENT_VALUES}, but '
                f'{environment!r} was passed'
            )            
        
    def load_context(self, environment, do_load_custom_expectations, search_start_dir = None, n_layers = 5):
        """
        Loads the YAML config file associated to the given `environment`, 
        starting in `search_start_dir` and moving upward, up to `n_layers`
        times.

        Parameters
        ----------
        environment: str, {'prod', `test'}
            the execution environment. if
            - test: development execution mode that does not upload the
                    data docs to the S3 bucket nor send slack alerts
            - prod: production execution mode that uploads the data docs
                    to the S3 bucket, sends slack alerts and stops the 
                    ETL process if the critical expectations are not met
        do_load_custom_expectations: bool
            whether to import custom expectations from the 
            `great_expectations/plugins/custom_expectations` folder or not
        search_start_dir: str, optional
            folder to start searching the YAML context files. if not given,
            it defaults to the current working directory
        n_layers: int, optional
            number of directory layers to search upwards

        Returns
        -------
        context: great_expectations.DataContext, optional
            the context object loaded from the project context file 
        """
        # find base folder
        search_paths = [search_start_dir] if search_start_dir is not None else SEARCH_START_DIRS
        base_path = search_fp_obj(
            target_fp_obj = DATA_QUALITY_DIR, 
            search_paths = search_paths, 
            target_fp_obj_name = DATA_QUALITY_DIR_NAME, 
            n_levels = n_layers
        )

        # locate context file, according to the given environment
        context_yml_fp = find_context_yml_file(environment, base_path)
        context_yml_dir, context_yml_filename = os.path.split(context_yml_fp)

        # overwrite the default `GE_YML` (YAML context file name)
        # attr to locate the custom one
        data_context_loader = ge.DataContext
        setattr(data_context_loader, GE_DEFAULT_CONTEXT_ATTR, context_yml_filename)

        # load context info from the YAML file
        context = data_context_loader(context_root_dir = context_yml_dir)

        if do_load_custom_expectations:
            load_custom_expectations(base_path)

        return context
    
    @overwrite_defaults
    def add_datasource(
        self,
        datasource_yaml_config = DATASOURCE_YAML,
        datasource_name = None, 
        data_connector_name = None, 
        batch_identifier = None, 
        context = None
    ):
        """
        Adds a `great_expectations` Datasource to the current or given 
        context.

        See the official documentation for more info 
        https://docs.greatexpectations.io/docs/reference/datasources/

        Parameters
        ----------
        datasource_yaml_config: str, optional
            datasource configuration with YAML format and optionally formattable
        datasource_name: str, optional
            `great_expectations` datasource name. if not given, it uses
            `self.datasource_name`
        data_connector_name: str, optional
            `great_expectations` data connector name. if not given, it uses
            `self.data_connector_name`
        batch_identifier: str, optional
            `great_expectations` batch identifier name. if not given, it uses
            `self.batch_identifier`
        context: great_expectations.DataContext, optional
            the context object where to add the datasource 

        Returns
        -------
        context: great_expectations.DataContext
            the new context object with the datasource 
        """
        datasource_yaml_config_formatted = datasource_yaml_config.format(
            datasource_name = self.datasource_name, 
            data_connector_name = self.data_connector_name,
            batch_identifier = self.batch_identifier
        )
        self.context.add_datasource(
            **yaml.safe_load(datasource_yaml_config_formatted)
        )
        
        return self.context
    
    def _add_expectations(self, expectation_suite_name, expectations):
        """
        creates a new expectation suite `expectation_suite_name`, add 
        the given `expectations` to it and saves the expectation suite
        to the current context
        """
        suite = self.context.create_expectation_suite(
            expectation_suite_name = expectation_suite_name, 
            overwrite_existing = True
        )  

        for expectation_data in expectations:
            suite.add_expectation(
                expectation_configuration = ExpectationConfiguration(
                    **expectation_data
            ))

        self.context.save_expectation_suite(
            expectation_suite = suite, 
            expectation_suite_name = expectation_suite_name
        )

        return self.context

    def _add_checkpoint(
        self, 
        checkpoint_name, 
        datasource_name,
        data_connector_name,
        data_asset_name,
        expectation_suite_name
    ):
        """adds a new checkpoint `checkpoint_name` to the current context"""
        checkpoint_yaml = CHECKPOINT_YAML.format(
            checkpoint_name = checkpoint_name
        )

        validations = [{
            'batch_request': {
                'datasource_name': datasource_name,
                'data_connector_name': data_connector_name,
                'data_asset_name': data_asset_name
            },
           'expectation_suite_name': expectation_suite_name
        }]

        self.context.add_checkpoint(
            **yaml.safe_load(checkpoint_yaml), 
            validations = validations
        )

        return self.context
    
    def _add_validation(self, validation_infix, validation_expectations):
        """adds an expectation suite and its associated checkpoint"""
        expectation_suite_name = self.expectation_suite_name.format(validation_infix)
        checkpoint_name = self.checkpoint_name.format(validation_infix)
        run_name = self.run_name.format(validation_infix)
        
        self._add_expectations(
            expectation_suite_name, 
            validation_expectations
        )
        
        self._add_checkpoint(
            expectation_suite_name = expectation_suite_name,
            checkpoint_name = checkpoint_name, 
            datasource_name = self.datasource_name,
            data_connector_name = self.data_connector_name,
            data_asset_name = self.data_asset_name,
        )

        return {
            'expectation_suite_name': expectation_suite_name, 
            'checkpoint_name': checkpoint_name, 
            'run_name': run_name
        }
        
    @overwrite_defaults
    def add_validations(
        self,
        critical_expectations, 
        warning_expectations,
        expectation_suite_name = None, 
        checkpoint_name = None, 
        datasource_name = None, 
        data_connector_name = None, 
        data_asset_name = None,
        context = None, 
    ):
        """
        Adds both the critical and the warning validations, i.e., 
        expectations suites and their corresponding checkpoints.

        See the official documentation for more info 
        https://docs.greatexpectations.io/docs/reference/expectation_suite_operations
        https://docs.greatexpectations.io/docs/reference/checkpoints_and_actions/

        Parameters
        ----------
        critical_expectations: list[dict]
            list of the critical expectations, defined as dictionaries
        warning_expectations: list[dict]            
            list of the warning expectations, defined as dictionaries
        expectation_suite_name: str, optional
            `great_expectations` expectation suite name. if not given, it uses
            `self.expectation_suite_name`
        checkpoint_name: str, optional
            `great_expectations` checkpoint name. if not given, it uses
            `self.checkpoint_name`
        datasource_name: str, optional
            `great_expectations` datasource name. if not given, it uses
            `self.datasource_name`
        data_connector_name: str, optional
             `great_expectations` data connector name. if not given, it uses
            `self.data_connector_name`
        data_asset_name: str, optional
            `great_expectations` data asset name. if not given, it uses
            `self.data_asset_name`
        context: great_expectations.DataContext, optional
            the context object where to add the datasource

        Returns
        -------
        context: great_expectations.DataContext
            the new context object with the validations
        validations_to_run: list[dict]
            list of dictionaries containing the expectation_suite_name,
            checkpoint_name and run_name of the validations to run

        Usage example
        --------------
        critical_expectations = [
            {
                'expectation_type': 'expect_column_values_to_not_be_null',
                'kwargs': {
                    'column': 'table_pk_col',
                },
            },
            {
                'expectation_type': 'expect_column_values_to_be_unique',
                'kwargs': {
                    'column': 'table_pk_col',
                },
            },
        ]
        warning_expectations = [
            {
                'expectation_type': 'expect_column_values_to_match_regex',
                'kwargs': {
                    'column': 'cups',
                    'regex': r'^ES\d{16}[A-Z]{2}\d?[CFPRXYZ]?$'
                },
            },
        ]

        checker = DataQualityChecker(
            base_name = 'example_table',
            environment = 'test'
        )

        checker.add_datasource()

        context, validations_to_run = checker.add_validations(
            critical_expectations = critical_expectations,
            warning_expectations = warning_expectations
        )
        """
        if not (critical_expectations or warning_expectations):
            raise ValueError(
                'you need to pass at least one of `critical_expectations` or `warning_expectations`'
            )

        validations_to_run = []
        if critical_expectations:
            validation_data = self._add_validation(
                validation_infix = CRITICAL_RESOURCE_INFIX,
                validation_expectations = critical_expectations
            )
            validations_to_run += [validation_data]
        else:
            logger.warning('Setting data quality validations without critical expectations')

        if warning_expectations:
            validation_data = self._add_validation(
                validation_infix = WARNING_RESOURCE_INFIX,
                validation_expectations = warning_expectations
            )
            validations_to_run += [validation_data]
        else:
            logger.warning('Setting data quality validations without warning expectations')
        
        self.checkpoints_to_run = [
            validation_data['checkpoint_name'] for validation_data in validations_to_run
        ]
        
        return self.context, validations_to_run
    
    @use_defaults
    def run_checkpoint(
        self, 
        df, 
        checkpoint_name, 
        batch_identifier = None, 
        run_name = None, 
        environment = None,
        context = None, 
    ):
        """
        Runs a the checkpoint `checkpoint_name` against the data in `df`
        and checks if the correspoding expectations are met.

        Parameters
        ----------
        df: pandas.DataFrame
            the in-memory data whose quality is checked
        checkpoint_name: str
            the name of the checkpoint to be ran
        batch_identifier: str, optional
            `great_expectations` batch identifier name. if not given, it uses
            `self.batch_identifier`
        run_name: str, optional 
            `great_expectations` checkpoint run name. if not given, it uses
            `self.run_name`
        environment: str, {'prod', `test'}
            the execution environment. if not given, it uses `self.environment`
        context: great_expectations.DataContext, optional
            the context object where to add the datasource        

        Returns
        -------
        result: great_expectations.checkpoint.types.checkpoint_result.CheckpointResult
            the new context object with the validations
        """
        if self.checkpoints_to_run is None:
            raise ValueError(
                'You must run `self.add_validations` to add the '
                'Expectations suite(s) and Checkpoint(s) before '
                'running the validations via this method.'
            )
        if checkpoint_name not in self.checkpoints_to_run:
            raise ValueError(
                f'There is no Checkpoint named {checkpoint_name!r}, '
                f'the available ones are {self.checkpoints_to_run}'
            )
        
        return self.context.run_checkpoint(
            checkpoint_name = checkpoint_name,
            batch_request = {
                'runtime_parameters': {'batch_data': df},
                'batch_identifiers': {
                    self.batch_identifier: self.environment
                },
            },
            run_name = run_name,
            run_time = datetime.datetime.now(pytz.timezone(TIMEZONE))
        ) 
    
    def _get_slack_notification_config(self, notify_on):
        """validates and gets the slack notification configuration from `notify_on`"""
        if isinstance(notify_on, dict):
            if not all(key in ACCEPTED_NOTIFY_ON_KEYS for key in notify_on.keys()):
                raise ValueError(
                    f'if passed as a dict, the keys of `notify_on` must '
                    f'be one of {ACCEPTED_NOTIFY_ON_KEYS}, but '
                    f'{list(notify_on.keys())} were passed'
                )
            if not all(value in ACCEPTED_NOTIFY_ON_VALUES for value in notify_on.values()):
                raise ValueError(
                    f'the accepted values of `notify_on` are '
                    f'{ACCEPTED_NOTIFY_ON_VALUES}, but '
                    f'{list(notify_on.values())} were passed'
                )

            notify_on_critical = notify_on.get('critical', CRITICAL_VALIDATION_NOTIFY_VALUE)
            notify_on_warning = notify_on.get('warning', WARNING_VALIDATION_NOTIFY_VALUE)
            
        else: # assume unique str value
            if notify_on not in ACCEPTED_NOTIFY_ON_VALUES:
                raise ValueError(
                    f'the accepted values of `notify_on` are {ACCEPTED_NOTIFY_ON_VALUES}, '
                    f' but {notify_on!r} was passed'
                )

            notify_on_critical = notify_on_warning = notify_on

        return notify_on_critical, notify_on_warning

    def _get_validation_status(self, result):
        """gets the validation status as string from the checpoint run result"""
        return SUCCESSFUL_VALIDATION_STR if result.success else FAILED_VALIDATION_STR
    
    def _send_slack_notification(
        self,
        result, 
        data_asset_name, 
        expectation_suite_name, 
        checkpoint_name,
        data_steward_name,
        environment,
        slack_credentials_key = SLACK_CREDENTIALS_KEY,
        slack_channel = SLACK_CHANNEL
    ):
        """
        Renders the results into a markdown-formatted string and sends it via the
        Slack API it to the given `slack_channel`
        """
        data_docs_site = ENVIRONMENT2DATA_DOCS_SITE_MAP[self.environment]
        validation_status = self._get_validation_status(result)
        validation_status_emoji = VALIDATION_STATUS2_EMOJI_MAP[validation_status]

        run_id = result.run_id.__repr__()
        batch_id = result.list_batch_identifiers()[0]

        # there is only one `validation_result` in `result['run_results']`
        # but this is the easiest way to get the data
        for validation_result in result.run_results.values():
            validation_statistics = validation_result['validation_result']['statistics']
            n_evaluated_expectations = validation_statistics['evaluated_expectations']
            n_successful_expectations = validation_statistics['successful_expectations']
            pct_successful_expectations = validation_statistics['success_percent'] 
            data_docs_url = validation_result['actions_results']['update_data_docs'][data_docs_site]


        slack_notification = SLACK_NOTIFICATION_TEMPLATE.format(
            validation_status = validation_status.title(),
            validation_status_emoji = validation_status_emoji,
            data_asset_name = data_asset_name,
            expectation_suite_name = expectation_suite_name,
            checkpoint_name = checkpoint_name,
            run_id = run_id,
            batch_id = batch_id,
            n_successful_expectations = n_successful_expectations,
            n_evaluated_expectations = n_evaluated_expectations,
            pct_successful_expectations = pct_successful_expectations,
            data_docs_url = data_docs_url,
            location = ENVIRONMENT2DATA_DOCS_LOCATION_NAME[environment]
        )

        if self.environment == PROD_ENVIRONMENT_NAME:
            send_message_by_slack(
                slack_notification, 
                load_credentials(slack_credentials_key), 
                script_name = __file__, 
                channel = slack_channel, 
                link_names = True,
                names_to_link = data_steward_name,
                add_emoji = False,
                verbose = False
            )
        else: # assume environment == TEST_ENVIRONMENT_NAME
            logger.info(slack_notification)
    
    @use_defaults
    def manage_validations_results(
        self,
        results, 
        validations_data,
        data_steward_name,
        context = None,
        notify_on = None,
        data_asset_name = None,
        environment = None,
    ):
        """
        Manages the validations results by sending slack notifications, according 
        to `notify_on`; putting the generated data docs on the local filesystem or
        on S3, depending on `environment` and stopping the ETL process if the critical
        expectations are not met.

        Parameters
        ----------
        results: list[CheckpointResult]
            a list of result object, as returned by `self.run_checkpoint`
        validations_data: list[dict]
            a list of dicts, as returned by `self.add_validations`
        notify_on: str or dict, optional
            the  notification mode. if not given, it uses `self.notify_on`
        data_asset_name: str, optional
            `great_expectations` data asset name. if not given, it uses
            `self.data_asset_name`
        environment: str, {'prod', `test'}
            the execution environment. if not given, it uses `self.environment`
        """
        notify_on_critical, notify_on_warning = self._get_slack_notification_config(notify_on)

        critical_expectations_success_flag = warning_expectations_success_flag = None
        for result, validation_data in zip(results, validations_data):
            validation_status = self._get_validation_status(result)
            
            expectation_suite_name = validation_data['expectation_suite_name']
            checkpoint_name = validation_data['checkpoint_name']
            
            if CRITICAL_RESOURCE_INFIX in checkpoint_name:
                critical_expectations_success_flag = result.success

                if notify_on_critical in ['all', validation_status]:
                    self._send_slack_notification(
                        result = result, 
                        data_asset_name = data_asset_name,
                        expectation_suite_name = expectation_suite_name, 
                        checkpoint_name = checkpoint_name,
                        data_steward_name = data_steward_name,
                        environment = environment
                    )
            elif WARNING_RESOURCE_INFIX in checkpoint_name:
                warning_expectations_success_flag = result.success

                if notify_on_warning in ['all', validation_status]:
                    self._send_slack_notification(
                        result = result, 
                        data_asset_name = data_asset_name,
                        expectation_suite_name = expectation_suite_name, 
                        checkpoint_name = checkpoint_name,
                        data_steward_name = data_steward_name,
                        environment = environment
                    )
            else:
                logger.warning(
                    f'Found unexpected ending for the Checkpoint {checkpoint_name}. '
                    f'Unable to send Slack notifications'
                )

        if warning_expectations_success_flag is not None and not warning_expectations_success_flag:
            logger.warning(
                'Warning-level data quality expectations failed'
            )

        if critical_expectations_success_flag is not None and not critical_expectations_success_flag:
            error_msg = 'Critical-level data quality expectations failed. Stopping ETL task.'
            logger.error(error_msg)
            
            if environment == PROD_ENVIRONMENT_NAME:
                raise CriticalExpectationsFailedError(error_msg)
        
def check_data_quality(
    df, 
    expectations_file,
    base_name, 
    environment,
    data_steward_name,
    *, 
    datasource_name = DATASOURCE_NAME, 
    data_connector_name = DATA_CONNECTOR_NAME, 
    batch_identifier = None,
    expectation_suite_name = None,
    checkpoint_name = None,
    data_asset_name = None,
    notify_on = 'failure',
    run_name = None, 
    search_start_dir = None,
    n_layers = 5,
    do_load_custom_expectations = False
):
    """
    High-level function that wraps `DataQualityChecker` to do
    an opinionated data quality check over the given in-memory
    data `df`. It automatically handles all `great_expectations`
    underlying objects, stores the data docs and validations
    either on the filesystem or in S3 and sends the corresponding
    slack alerts (depending on `environment`).
    
    Parameters
    ----------
    df: pandas.DataFrame
        the in-memory data whose quality is checked
    expectations_file: str
        name of the expectations file, that can optionally contain
        a path part too.
        NOTE: the actual filepath to the expectations file is built
        as `os.path.join(search_start_dir, expectations_file)`
    base_name: str
        base name of the underlying `great_expectations` objects.
        we recommend to set it as the table name whose data is 
        about to be checked
    environment: str, {'prod', `test'}
        the execution environment. if
        - test: development execution mode that does not upload the
                data docs to the S3 bucket nor send slack alerts
        - prod: production execution mode that uploads the data docs
                to the S3 bucket, sends slack alerts and stops the 
                ETL process if the critical expectations are not met
    data_steward_name: str
            slack user name of the data steward responsible of the
            given data quality
    datasource_name: str, optional
        `great_expectations` datasource name. if not given, it defaults
        to `DATASOURCE_NAME`
    data_connector_name: str, optional
        `great_expectations` data connector name. if not given, it defaults
        to `DATA_CONNECTOR_NAME`
    batch_identifier: str, optional
        `great_expectations` batch identifier name. if not given, it defaults
        to a combination of `BATCH_IDENTIFIER_PREFIX` and `base_name`
    expectation_suite_name: str, optional
        `great_expectations` expectation suite name. if not given, it defaults
        to a combination of `base_name` and `EXPECTATION_SUITE_NAME_SUFFIX`
    checkpoint_name: str, optional
        `great_expectations` checkpoint name. if not given, it defaults
        to a combination of `base_name` and `CHECKPOINT_NAME_SUFFIX`
    data_asset_name: str, optional
        `great_expectations` data asset name. if not given, it defaults
        to a combination of `base_name` and `DATA_ASSET_NAME_SUFFIX`
    run_name: str, optional 
        `great_expectations` checkpoint run name. if not given, it defaults
        to a combination of `base_name` and `CHECKPOINT_RUN_NAME_SUFFIX`
    notify_on: str or dict, optional
        notification mode. if
        - `str`: it must be one of `ACCEPTED_NOTIFY_ON_VALUES`
        - `dict`: its keys must belong to `ACCEPTED_NOTIFY_ON_KEYS`
                  and its values be one of `ACCEPTED_NOTIFY_ON_VALUES`
        it defaults to `CRITICAL_VALIDATION_NOTIFY_VALUE`
    search_start_dir: str, optional
        folder to start searching the YAML context files. If None, it is
        determined using `_find_base_path`
    n_layers: int, optional
        number of directory layers to search upwards
    do_load_custom_expectations: bool, optional
        whether to import custom expectations from the 
        `great_expectations/plugins/custom_expectations` folder or not
    """
    expectations = load_expectations_file(
        expectations_file = expectations_file, 
        base_path = search_start_dir, 
        n_layers = n_layers
    )
    
    checker = DataQualityChecker(
        base_name = base_name,
        datasource_name = datasource_name, 
        data_connector_name = data_connector_name,
        batch_identifier = batch_identifier,
        expectation_suite_name = expectation_suite_name,
        checkpoint_name = checkpoint_name,
        data_asset_name = data_asset_name,
        notify_on = notify_on,
        run_name = run_name, 
        environment = environment,
        search_start_dir = search_start_dir,
        n_layers = n_layers,
        do_load_custom_expectations = do_load_custom_expectations
    )
    
    checker.add_datasource()

    _, validations_data = checker.add_validations(
        critical_expectations = expectations.get('critical_expectations', []), 
        warning_expectations = expectations.get('warning_expectations', [])
    )
    
    results = [
        checker.run_checkpoint(
            df = df, 
            checkpoint_name = validation_data['checkpoint_name'], 
            run_name = validation_data['run_name'], 
        )
        for validation_data in validations_data
    ]
    
    checker.manage_validations_results(
        results = results, 
        validations_data = validations_data, 
        data_steward_name = data_steward_name
    )
    
    return checker.context, results
