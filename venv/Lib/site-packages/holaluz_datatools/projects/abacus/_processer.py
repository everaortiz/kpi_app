import logging
import re
import pandas as pd

from ...sql import PostgreSQLClient, SQLServerClient, MySQLClient, SnowflakeSQLClient
from ...aws import S3Resource
from ...credentials import load_credentials
from ._parser import MetricParser
from ._notion import NotionWrapper
from ._params import *


logger = logging.getLogger(__name__)

class MetricProcesser:
    """
    Processer class that computes the metrics (or alerts) according to
    the local and S3 files. It uses `MetricParser` to read the files 
    and `NotionWrapper` to create the Notion pages, when necessary.
    """
    def __init__(
        self,
        mode = DEFAULT_MODE,
        local_path = None, 
        files_extension = None,
        credentials_fp = None,
        notion_creds_key = NOTION_CREDS_KEY,
        s3_creds_key = S3_CREDS_KEY,
        s3_bucket = S3_BUCKET,
        s3_path = None,
        table_name = None
    ):
        """
        Parameters
        ----------
        mode: str, optional
            whether to use the wrapper to interact with
                - the BI Metrics page (if `mode = 'metrics'`)
                - the BI Alerts page (if `mode = 'alerts'`)
        local_path: str, optional
            path where the metric files are located. if not given,
            it uses the default according to `MODE2PAGE_DATA_MAP`
            and `mode`
        files_extension: str, optional
            default metric files extensions. if not given,
            it uses the default according to `MODE2PAGE_DATA_MAP`
            and `mode`
        credentials_fp: str, optional
            path (and file name) the credentials fp
        notion_creds_key: str, optional
            the Notion credentials key, which is passed
            to `load_credentials`
        s3_creds_key: str, optional
            the S3 credentials key, which is passed
            to `load_credentials`
        s3_bucket: str, optional
            name of the S3 bucket. if its boolean value is False,
            the processer does not retrieve the additional metric
            info from S3
        s3_path: str, optional
            path inside the `s3_bucket` to look for the metrics.
            if not given,it uses the default according to 
            `MODE2PAGE_DATA_MAP` and `mode`
        table_name: str
            Snowflake table where the metrics info is inserted
            (when using `self.insert_metrics`). if not given,
            it uses the default according to `MODE2PAGE_DATA_MAP`
            and `mode`
        """
        if mode not in MODE_ACCEPTED_VALUES:
            raise ValueError(f'mode must be one of {MODE_ACCEPTED_VALUES}')
        
        self.mode = mode
        self.operation = MODE2PAGE_DATA_MAP[mode]['operation']
        
        self.local_path = local_path or MODE2PAGE_DATA_MAP[mode]['local_path']
        self.files_extension = files_extension or MODE2PAGE_DATA_MAP[mode]['files_extension']
        self.metric_parser = MetricParser(
            mode = mode,
            local_path = self.local_path, 
            files_extension = self.files_extension
        )
        
        self.credentials_fp = credentials_fp
        credentials = load_credentials(credentials_fp = self.credentials_fp)

        self.s3_bucket = s3_bucket
        self.s3_path = s3_path or MODE2PAGE_DATA_MAP[mode]['s3_path']
        self.s3_resource = S3Resource(**credentials[s3_creds_key])

        self.notion_wrapper = NotionWrapper(**credentials[notion_creds_key], mode = mode)
        
        self.table_name = table_name or MODE2PAGE_DATA_MAP[mode]['table_name']
    
    def compute_sql_metric(self, metric_data):
        """
        Computes the SQL query from `metric_data`
        
        Parameters
        ----------
        metric_data: dict
            the metric data as a dictionary, i.e., each value
            of what the `MetricParser.read_metric_file` returns
        
        Returns
        -------
        metric: pandas.DataFrame
        """
        SQLClient = eval(SQL_FLAVOUR2CLIENT_MAP[metric_data['flavour']])
        with SQLClient(**load_credentials(metric_data['credentials_key'])) as sql_client:
            logger.info(f'running SQL query')
            return sql_client.make_query(metric_data['query'])
        
    def process_metric(self, metric):
        """
        Processes the input metric `metric`, i.e., adds it to 
        the BI Metrics Notion page (if necessary) and computes
        the corresponding SQL query
        
        Parameters
        ----------
        metric_data: dict
            the metric data as a dictionary, i.e., each value
            of what the `MetricParser.read_metric_file` returns
        
        Returns
        -------
        metric: pandas.DataFrame
        """
        dfs = []
        for section_name, metric_data in self.metric_parser.get_metric_data(metric).items():
            logger.info(f'processing section {section_name!r} from {self.operation} {metric_data["name"]!r}')
            
            self.notion_wrapper.add_metric(metric_data)
            dfs.append(self.compute_sql_metric(metric_data))
            
        return pd.concat(dfs, axis = 1)
            
    def process_metrics_from_files(self, local_path = None, pattern = None):
        """
        Processes the metric files from  `local_path`, i.e., 
        adds them to the BI Metrics Notion page (if necessary)
        and computes the corresponding SQL queries
        
        Parameters
        ----------
        local_path: str, optional
            the path where the metric files are stored. if not given, 
            it uses `self.local_path`
        pattern: str, optional
            regex pattern to filter the read metrics. if not given, 
            it reads all metric files in `local_path`
        
        Returns
        -------
        metrics: pandas.DataFrame
        """
        local_path = local_path or self.local_path

        df = pd.concat(
            (self.process_metric(metric) for metric in self.metric_parser.read_metric_files(local_path, pattern)),
            axis = 1
        )
        return df.assign(date = pd.to_datetime('today').date())
    
    def process_metrics_from_s3(self, s3_bucket = None, s3_path = None, **kwargs):
        """
        Processes the metrics from S3, i.e., pulls the processed metrics 
        and joins them into a single `pandas.DataFrame`
        
        Parameters
        ----------
        s3_bucket: str, optional
            the bucket where the processed metric files are stored. if not
            given, it uses `self.s3_bucket`
        s3_path: str, optional
            the path where the processed metric files are stored. if not
            given, it uses `self.s3_path`
        kwargs:
            other keyword argument forwarded to `S3Resource` to have more
            control over the pulled files
        
        Returns
        -------
        metrics: pandas.DataFrame
        """
        if (s3_bucket or self.s3_bucket) is None:
            logger.warning(f'skipping potential metrics from S3')
            return pd.DataFrame([])
        
        logger.info(f'processing metrics from S3 bucket {s3_bucket!r}')
        kwargs = {**kwargs, 'concat_axis': 1} # overwrite `concat_axis` if given, to make sure the retured df is a single row
        # TODO: check df shape
        return self.s3_resource.pull_from_s3(s3_bucket or self.s3_bucket, s3_path or self.s3_path, **kwargs)
        
    def process_metrics(self, local_path = None, pattern = None, s3_bucket = None, s3_path = None, **kwargs):
        """
        Processes the metric files from `local_path` (i.e., 
        adds them to the BI Metrics Notion page, if necessary,
        and computes the corresponding SQL queries) and gets the
        processed metric files from S3. Then, joins all metrics 
        info into one single `pandas.DataFrame`
        
        Parameters
        ----------
        local_path: str, optional
            the path where the metric files are stored. if not given, 
            it uses `self.local_path`
        pattern: str, optional
            regex pattern to filter the read metrics. if not given, 
            it reads all metric files in `local_path`
        s3_bucket: str, optional
            the bucket where the processed metric files are stored. if not
            given, it uses `self.s3_bucket`
        s3_path: str, optional
            the path where the processed metric files are stored. if not
            given, it uses `self.s3_path`
        kwargs:
            other keyword argument forwarded to `S3Resource` to have more
            control over the pulled files
        
        Returns
        -------
        metrics: pandas.DataFrame
        """
        df_files = self.process_metrics_from_files(local_path, pattern)
        df_s3 = self.process_metrics_from_s3(s3_bucket, s3_path, **kwargs)
        df = pd.concat([df_files, df_s3], axis = 1)
        return df.reindex(columns = ['date'] + sorted(df.columns[df.columns != 'date']))

    def insert_metrics(self, metrics_df, sf_credentials_key, credentials_fp = None, schema = SF_SCHEMA, table_name = None):
        """
        Inserts the daily metrics into the corresponding table in 
        Snowflake data warehouse
        
        Parameters
        ----------
        metrics_df: pandas.DataFrame
            a dataframe containing the metrics data, e.g., the output of
            `self.process_metrics`
        sf_credentials_key: str
            the Snowflake credentials key, which is passed
            to `load_credentials`
        credentials_fp: str, optional
            path (and file name) the credentials fp; if not given, it
            uses `self.credentials_fp`
        schema: str, optional
            Snowflake schema that contains the table where the metrics
            info is inserted
        table_name: str, optional
            name of the Snowflake table where the metrics data is inserted.
            if not given, it uses `self.table_name`
        """
        table_name = table_name or self.table_name
        
        with SnowflakeSQLClient(**load_credentials(sf_credentials_key, credentials_fp or self.credentials_fp)) as sf_client:
            if sf_client.table_exists(table_name):
                logger.info(f'table {table_name!r} already exists in schema {schema!r}')
                current_col_names = sf_client.get_table_col_names(table_name)
                if metrics_df.columns.equals(current_col_names):
                    sf_client.write_table(metrics_df, table_name, schema, if_exists = 'append')
                else:
                    logger.warning(f'the data from table {table_name!r} and the current data have different columns')
                    current_table = sf_client.make_query(F"SELECT * FROM DATA_WAREHOUSE.{schema}.{table_name}")
                    metrics_df = pd.concat([current_table, metrics_df])
                    sf_client.write_table(metrics_df, table_name, schema, if_exists = 'replace')
            else:
                sf_client.write_table(metrics_df, table_name, schema)
                
    def _handle_error(self, error, mode):
        """controls how an error is handled"""
        if mode == 'raise':
            logger.error(f'{type(error).__name__}: {error}')
            raise error
        elif mode == 'show':
            logger.exception(f'{type(error).__name__}: {error}')
        elif mode == 'ignore':
            pass

    def _modify_query_date_filter(self, query, date, date_col = None, errors = 'raise'):
        """modifies the query `query` by adjusting its temporal filter"""
        if not re.search('where', query, flags = re.IGNORECASE):
            if not date_col:
                ve = ValueError(
                    f'the query\n""""{query}"""\n does not contain any `WHERE` statement. '
                    f'thus, to filter according to the given date {date!r}, the user must '
                    f'set the `date_col` argument, which is now \'{date_col}\''
                )
                self._handle_error(ve, mode = errors)
                
            if not re.search('group by', query, flags = re.IGNORECASE):
                query += f'\nwhere cast({date_col} as date) = {date!r}'
            else:
                query_pre, query_post = query.lower().rsplit('group by', maxsplit = 1)
                query = query_pre + f'\nwhere cast({date_col} as date) = {date!r}\n + group by' + query_post
            
        else:
            # add extra whitespace before the comparison operators, for the following regex to match correctly
            operator_search_set = set(re.findall('([<>=]{1,2})', query))
            if operator_search_set:
                for match in operator_search_set:
                    query = query.replace(match, f' {match} ')

                # search the query date filter and replace it, if possible
                search = re.search(DATE_FILTER_PATTERN, query, flags = re.IGNORECASE)
                if search:
                    query = query.replace(search.groups()[0], f'= {date!r}')
                else:
                    ve = ValueError(f'Unable to find a matching date filter for the query\n"""{query}"""')
                    self._handle_error(ve, mode = errors)
            else:
                ve = ValueError(f'the query\n""""{query}"""\ndoes not contain any comparison filter')
                self._handle_error(ve, mode = errors)

        return query

    def update_metric(
        self,
        metric_name, 
        date, 
        sf_credentials_key, 
        date_col = None, 
        local_path = None, 
        credentials_fp = None,
        schema = SF_SCHEMA, 
        table_name = None
    ):
        """
        Updates the value of the metric `metric_name` for the given date `date`.
        Note that if `date` is not present in the metrics table, no value will
        be updated.

        Parameters
        ----------
        metric_name: str
            metric name or file name
        date: str
            the date where the metric `metric_name` has to be updated.
            it must be formatted as `'YYYY-MM-DD'`
        sf_credentials_key: str
            the Snowflake credentials key, which is passed
            to `load_credentials`
        date_col: str, optional
            name of a date column used to filter by the given date `date`.
            only necessary if the metric query has no `WHERE` statement
        local_path: str, optional
            the path where the metric files are stored. if not given, 
            it uses `self.local_path`
        credentials_fp: str, optional
            path (and file name) the credentials fp. if not given, 
            it uses `self.credentials_fp`
        schema: str, optional
            Snowflake schema that contains the table where the metrics
            info is updated
        table_name: str, optional
            name of the Snowflake table where the metrics data is updated.
            if not given, it uses `self.table_name`
        """
        assert re.match(DATE_PATTERN, date),\
            f'date must be formatted as {DATE_FORMAT!r}, prior to {MIN_ALLOWED_DATE} and subsequent to {MAX_ALLOWED_DATE}'

        metric = self.metric_parser.read_metric_file(metric_name, local_path or self.local_path)

        data_tuples_list = []
        for section_name, metric_data in self.metric_parser.get_metric_data(metric).items():
            logger.info(
                f'updating section {section_name!r} from {self.operation} '
                f'{metric_data["name"]!r} for date {date!r}'
            )

            metric_data['query'] = self._modify_query_date_filter(metric_data['query'], date, date_col)
            logger.debug(metric_data['query'])

            df_metric = self.compute_sql_metric(metric_data)

            col_name = df_metric.columns[0]
            value = df_metric[col_name][0]
            data_tuples_list.append((col_name, value))

        # join the metrics data as the `UPDATE` statement requires, i.e.,
        # as <col_name> = <value> [, <col_name> = <value>, ...]
        data_str = ', '.join(' = '.join(map(str, t)) for t in data_tuples_list)
        query = UPDATE_METRIC_QUERY.format(
            schema = schema, 
            table_name = table_name or self.table_name, 
            data_str = data_str, 
            date = date
        )
        with SnowflakeSQLClient(**load_credentials(sf_credentials_key, credentials_fp or self.credentials_fp)) as sf_client:
            return sf_client.make_query(query)